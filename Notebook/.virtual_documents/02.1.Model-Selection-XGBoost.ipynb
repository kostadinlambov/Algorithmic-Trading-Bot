import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from functools import partial

import pickle
import mlflow
import time
import xgbfir

import optuna
import optuna.visualization as vis

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV

# Scalers
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE, ADASYN

# Metrics
from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, precision_recall_curve, f1_score

# Model
from xgboost import XGBClassifier, plot_importance








attributes = pd.read_csv("data/btc_price_data_1_year_attributes.csv")
attributes.set_index('timestamp', inplace=True, drop = True)
attributes


target_data = pd.read_csv("data/btc_price_data_1_year_target.csv")
target_data.set_index('timestamp', inplace=True, drop = True)
target = target_data['target']
target





attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape














def create_oversampling_smote(attributes_train, target_train):
    """
    Apply SMOTE(Synthetic Minority Oversampling Technique) to generate synthetic examples for minority classes
    """
    smote = SMOTE(random_state=42)
    attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (SMOTE):", np.bincount(target_train))
    print("Balanced Class Distribution (SMOTE):", np.bincount(target_train_smote))

    return attributes_train_smote, target_train_smote





def create_oversampling_adasyn(attributes_train, target_train):
    """
    Apply ADASYN(Adaptive Synthetic Sampling) to generate synthetic examples for minority classes
    """
    adasyn = ADASYN(random_state=42)
    attributes_train_adasyn, target_train_adasyn = adasyn.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (ADASYN):", np.bincount(target_train))
    print("Balanced Class Distribution (ADASYN):", np.bincount(target_train_adasyn))

    return attributes_train_adasyn, target_train_adasyn





# Set oversampling type
oversampling_type = 'adasyn'


def create_oversampling(attributes_train, target_train, oversampling_type = None):
    """
    Apply 'oversampling_type' to generate synthetic examples for minority classes
    """
    if oversampling_type == 'smote':
        return create_oversampling_smote(attributes_train, target_train)
    elif oversampling_type == 'adasyn':
        return create_oversampling_adasyn(attributes_train, target_train)
    else:
        print(f"Invalid oversampling type: [{oversampling_type}]")
        return attributes_train, target_train











initial_xgboost_model_parameters = {
    'objective': 'multi:softmax',                                  # Multiclass classification
    'num_class': 3,                                                # Number of target classes
    'eval_metric': ['mlogloss', 'merror'],                         # Evaluation metric
    'max_depth': 6,                                                # Controls model complexity
    'learning_rate': 0.1,                                          # Step size for weight updates
    'n_estimators': 300,                                           # Number of boosting rounds
    'subsample': 0.8,                                              # Fraction of data used for training
    'colsample_bytree': 0.8,                                       # Fraction of features used
    'reg_alpha': 0.5,                                                # L1 regularization
    'reg_lambda': 0.07,                                              # L2 regularization
    'min_child_weight': 2,
    #'early_stopping_rounds': 10,                                   # Stop if no improvement in 10 rounds
    'verbosity': 3,
    'n_jobs': 10,
    'random_state': 42
}








def objective(trial, oversampling_type = None, n_splits = 5):
    print(f"Start trial:[{trial.number}] with time series cross validation, oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")

    # Define the hyperparameters to tune
    params = {
        'objective': 'multi:softmax',                                                # Multiclass classification
        'num_class': 3,                                                              # Number of target classes
        'eval_metric': ['mlogloss', 'merror'],                                       # Evaluation metric
        'max_depth': trial.suggest_int('max_depth', 3, 10),                          # Tree depth - controls model complexity
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log = True),  # Step size for weight updates
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),                 # Number of boosting rounds
        # 'n_estimators': trial.suggest_int('n_estimators', 100, 500),               # Number of boosting rounds
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),                     # Row sampling - fraction of data used for training
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),       # Feature sampling - fraction of features used
        # 'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 50),        # Class imbalance - for  a multiclass classification  this parameter can't be applied.
        'reg_alpha': trial.suggest_float("reg_alpha", 0, 10),                        # L1 regularization
        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log = True),      # L2 regularization --> good values typically fall in [0,10]
        'min_child_weight': trial.suggest_int("min_child_weight", 1, 10),
        'random_state': 42,
        'early_stopping_rounds': 10,                                                  # Stop if no improvement in 10 rounds
        'n_jobs' : 9
    }

    # Use TimeSeriesSplit for time series cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    f1_scores = []  # Store F1-scores for each fold

    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes)):
        attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
        target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]

        # Apply oversampling to handle class imbalance in training data
        attributes_train_fold, target_train_fold = create_oversampling(
            attributes_train_fold,
            target_train_fold, 
            oversampling_type
        )

        # Create XGBClassifier with the params
        model = XGBClassifier(**params)
        
        # Train XGBoost model
        model.fit(attributes_train_fold,
                  target_train_fold,
                  eval_set=[(attributes_val_fold, target_val_fold)],
                  verbose=False)

        # Predictions and probabilities
        target_val_pred = rf_model_split.predict(attributes_val_fold)
        target_val_proba = rf_model_split.predict_proba(attributes_val_fold)

        # Calculate metrics for this fold
        f1_macro_scores.append(f1_score(target_val_fold, target_val_pred, average='macro'))
        f1_weighted_scores.append(f1_score(target_val_fold, target_val_pred, average='weighted'))
        balanced_acc_scores.append(balanced_accuracy_score(target_val_fold, target_val_pred))
        kappa_scores.append(cohen_kappa_score(target_val_fold, target_val_pred))
        log_loss_scores.append(log_loss(target_val_fold, target_val_proba))
        roc_auc_scores.append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))

        print(f"Fold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))
    
    # Return the mean F1-macro across all folds
    return np.mean(f1_macro_scores)





model_type = 'XGBClassifier'
n_trials = 3 # default : 50
n_splits = 3 # default : 5
posfix = ''
mlflow_run_name = f"BTC_{model_type}_Optuna_{oversampling_type}-{n_trials}-Trials{posfix}"


# Create a MLFlow experiment
experiment_name = f"BTC_{model_type}_Optuna"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None: 
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)


with mlflow.start_run(run_name = mlflow_run_name):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))

    # Create a Optuna study and optimize
    study = optuna.create_study(direction='maximize')
    study.optimize(
        partial(objective, oversampling_type = oversampling_type, n_splits = 5),
        n_trials = n_trials # adjust the number of trials as needed
    )

    # Visualize optimization history
    optimization_history_plot = vis.plot_optimization_history(study)
    # Visualize parameter importance
    param_importance_plot = vis.plot_param_importances(study)
    # Visualize hyperparameter values
    hyperparameter_values_plot = vis.plot_parallel_coordinate(study)

    # Save visualizations to files
    optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Log visualization MLflow artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_F1_macro-score", study.best_value)

    # Output the best hyperparameters
    print("Best Parameters:", study.best_params)
    print("Best F1-macro-score:", study.best_value)
    print("Best Trial:", study.best_trial)

    # Train the model with evaluation metrics
    eval_set = [(attributes_train, target_train), (attributes_test, target_test)]

    # Train best model
    merged_params = initial_xgboost_model_parameters | study.best_params

    print("merged_params:", merged_params)
    
    model_xgb = XGBClassifier(**merged_params)
    model_xgb.fit(attributes_train, target_train, eval_set = eval_set)

    # Validate
    target_train_pred = model_xgb.predict(attributes_train)
    classification_train_report = classification_report(target_train, target_train_pred)

    target_test_pred = model_xgb.predict(attributes_test)
    classification_test_report = classification_report(target_test, target_test_pred)

    print(f"classification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
    print(f"classification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)

    # Log metrics
    mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", model_xgb.score(attributes_train, target_train))
    mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", model_xgb.score(attributes_test, target_test))

    with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_train_report)
    with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_test_report)
        
    pickle.dump(model_xgb, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))

    # Log artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
    
    classes = [0, 1, 2]
    evaluate_model(model_xgb, attributes_test, target_test, classes)





param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 300],
    # 'n_estimators': [100, 300, 500],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'scale_pos_weight': [1, 10, 50]
}

# Use TimeSeriesSplit for time series cross-validation
tscv = TimeSeriesSplit(n_splits = n_splits)

grid_search = GridSearchCV(
    estimator=XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', random_state=42),
    param_grid=param_grid,
    scoring='f1_weighted',  # Use an appropriate scoring metric
    cv=tscv,                # Use TimeSeriesSplit
    verbose=4,
    n_jobs=9
)

# grid_search.fit(attributes_train_smote, target_train_smote)
# print("Best Parameters:", grid_search.best_params_)

# Output best parameters and score
# print("Best parameters:", grid_search.best_params_)
# print("Best accuracy:", grid_search.best_score_)

# Evaluate on test set
# best_model = grid_search.best_estimator_
# test_score = best_model.score(attributes_test, target_test)
# print("Test set accuracy:", test_score)








def print_classification_report_multiclass(model, attributes_test_data, target_test_data): 
    # Predictions
    target_pred_data = model.predict(attributes_test_data)

    # Calculate F1-score (weighted)
    f1 = f1_score(target_test_data, target_pred_data, average='weighted')

    print(f"F1 score (weighted) : [{f1}]")
    
    # Classification Report
    print("Classification report: ")
    print(classification_report(target_test_data, target_pred_data))


def plot_classification_error(model, eval_metric='mlogloss'):
    # Retrieve evaluation results
    eval_results = model.evals_result()
    
    # Plot classification error
    plt.figure(figsize=(10, 6))
    plt.plot(eval_results['validation_0'][eval_metric], label='Train Error')
    plt.plot(eval_results['validation_1'][eval_metric], label='Validation Error')
    plt.xlabel('Boosting Rounds')
    plt.ylabel('Classification Error')
    plt.title(f"Classification Error ({eval_metric}) During Training ")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_roc_curve_multiclass(model,target_test_data, classes):
    print(f"Calculate the ROC-AUC score for each class individually to understand how well the model performs for the minority classes")
    
    # Binarize labels for one-vs-rest (multiclass ROC-AUC)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities

    # ROC-AUC for multiclass
    roc_auc = roc_auc_score(target_test, target_pred_prob, multi_class='ovr') # 'ovr' (one-vs-rest), 'ovo'-(one-vs-one)
    print(f"ROC-AUC Score: {roc_auc}")
    
    # Compute ROC-AUC for each class
    roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
    print(f"ROC-AUC per class {classes}:", roc_auc_per_class)

    # Compute ROC curve and ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot ROC curves for each class
    plt.figure()
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    
    # Configure the plot
    plt.title('ROC-AUC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()


def plot_precision_recal_curve_multiclass(model,target_test_data, classes):
    # Binarize labels for one-vs-rest (multiclass precision-recall)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities
    
    # Plot precision-recall curves for each class
    for i in range(3):  # Assuming 3 classes (0, 1, 2)
        precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        plt.plot(recall, precision, label=f'Class {i}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()


def plot_feature_importance_xgb(model):
    plot_importance(model)


def evaluate_model(model, attributes_test_data, target_test_data, classes):
    print_classification_report_multiclass(model, attributes_test_data, target_test_data)
    plot_classification_error(model, eval_metric='mlogloss')
    plot_classification_error(model, eval_metric='merror')
    plot_roc_curve_multiclass(model, target_test_data, classes)
    plot_precision_recal_curve_multiclass(model, target_test_data, classes)
    plot_feature_importance_xgb(model)


classes = [0, 1, 2]
evaluate_model(model_xgb, attributes_test, target_test, classes)





def run_time_series_cross_validation(model, attributes_data, oversampling_type = None, n_splits = 5):
    """
    Runs a cross validation for time series data. The 'attributes_data' is splitted with a 'TimeSeriesSplit' in 'n_splits'-folds
    
    Parameters:
    ----------
    model : the xgb model that will be used for the cross validation
    
    attributes_data: attributes data for the cross validation
    
    n_splits : number of splis the data for the cross validation
    
    oversampling_type : applies oversampling to generate synthetic examples for minority classes. Possible values: 'adasyn' or 'smote'
    """ 
    
    print(f"Start time series cross validation with oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")
    
    # Use TimeSeriesSplit for cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes_data)):
        attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
        target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]
        
        attributes_train_fold, target_train_fold = create_oversampling(
            attributes_train_fold,
            target_train_fold, 
            oversampling_type = oversampling_type
        )
        
        # Train model 
        model.fit(attributes_train_fold,
                  target_train_fold,  
                  eval_set=[(attributes_val_fold, target_val_fold)],
                  verbose=False)
    
        # Validate
        target_val_pred = model.predict(attributes_val_fold)
        
        print(f"Fold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))


run_time_series_cross_validation(model_xgb, attributes, oversampling_type, n_splits = 5)



