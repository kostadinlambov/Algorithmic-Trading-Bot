import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import pickle
import mlflow
import time

import optuna

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV

# Scalers
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE

# Metrics
from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, precision_recall_curve, f1_score

# Model
from xgboost import XGBClassifier








attributes = pd.read_csv("data/btc_price_data_1_year_attributes.csv")
attributes.set_index('timestamp', inplace=True, drop = True)
attributes


target_data = pd.read_csv("data/btc_price_data_1_year_target.csv")
target_data.set_index('timestamp', inplace=True, drop = True)
target = target_data['target']
target





attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape





# Apply SMOTE to generate synthetic examples for minority classes
smote = SMOTE(random_state=42)
attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)

print("Original Class Distribution:", np.bincount(target_train))
print("Balanced Class Distribution:", np.bincount(target_train_smote))








# Initial XGBoost model with recommended parameters
model_xgb = XGBClassifier(
    objective='multi:softmax',  # Multiclass classification
    num_class=3,                # Number of target classes
    eval_metric='mlogloss',
    max_depth=6,                # Controls model complexity
    learning_rate=0.1,          # Step size for weight updates
    n_estimators=300,           # Number of boosting rounds
    subsample=0.8,              # Fraction of data used for training
    colsample_bytree=0.8,       # Fraction of features used
    scale_pos_weight=1,         # Placeholder for class imbalance
    random_state=42,
    verbosity = 3,
    n_jobs = 9
)


# Train the model
start_time = time.time()
model_xgb.fit(attributes_train_smote, target_train_smote)
print('Fit time : ', time.time() - start_time)





# Predictions
y_pred = model_xgb.predict(attributes_test)
# y_pred = model.predict(X_test)

# Classification Report
print(classification_report(target_test, y_pred))
# print(classification_report(y_test, y_pred))

# ROC-AUC for multiclass
y_prob = model_xgb.predict_proba(attributes_test)
roc_auc = roc_auc_score(target_test, y_prob, multi_class='ovr')
print(f"ROC-AUC Score: {roc_auc}")

# y_prob = model.predict_proba(X_test)
# roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')









# Binarize labels for one-vs-rest (multiclass ROC-AUC)
target_test_binarized = label_binarize(target_test, classes=[0, 1, 2])
target_pred_prob = model_xgb.predict_proba(attributes_test) # predicted probabilities

# Compute ROC-AUC for each class
roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
print("ROC-AUC per class [0, 1, 2]:", roc_auc_per_class)


# Binarize the labels for one-vs-rest classification
classes = [0, 1, 2]
target_test_binarized = label_binarize(target_test, classes=classes)

# Compute ROC curve and ROC-AUC for each class
fpr = {}
tpr = {}
roc_auc = {}

for i in range(len(classes)):
    fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves for each class
plt.figure()
for i in range(len(classes)):
    plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')

# Plot the diagonal
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')

# Configure the plot
plt.title('ROC-AUC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()








# Plot precision-recall curves for each class
for i in range(3):  # Assuming 3 classes (0, 1, 2)
    precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
    plt.plot(recall, precision, label=f'Class {i}')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()






# Use TimeSeriesSplit for cross-validation
tscv = TimeSeriesSplit(n_splits=5)
for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes)):
    attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
    target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]

    # Apply SMOTE to the training fold
    attributes_train_fold_smote, target_train_fold_smote = smote.fit_resample(attributes_train_fold, target_train_fold)

    # Train model
    model_xgb.fit(attributes_train_fold_smote, target_train_fold_smote)

    # Validate
    target_val_pred = model_xgb.predict(attributes_val_fold)
    print(f"Fold {fold+1} Classification Report")
    print(classification_report(target_val_fold, target_val_pred))





















def objective(trial):
    optuna.logging.set_verbosity(optuna.logging.DEBUG)
    
    # Define the hyperparameters to tune
    param = {
        'objective': 'multi:softmax',  # Multiclass classification
        'num_class': 3,               # Number of target classes
        'eval_metric': 'mlogloss',    # Evaluation metric
        'max_depth': trial.suggest_int('max_depth', 3, 10),  # Tree depth
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),  # Step size
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),  # Boosting rounds
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),  # Row sampling
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),  # Feature sampling
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 50),  # Class imbalance
        'random_state': 42
    }
    
    # Use TimeSeriesSplit for time series cross-validation
    tscv = TimeSeriesSplit(n_splits=5)
    
    f1_scores = []  # Store F1-scores for each fold
    
    for train_idx, val_idx in tscv.split(attributes):
        attributes_train, attributes_val = attributes.iloc[train_idx], attributes.iloc[val_idx]
        target_train, target_val = target.iloc[train_idx], target.iloc[val_idx]

        # Apply SMOTE to handle class imbalance in training data
        smote = SMOTE(random_state=42)
        attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)

        # Train XGBoost model
        model = XGBClassifier(**param)
        model.fit(attributes_train_smote, target_train_smote)

        # Make predictions
        target_pred = model.predict(attributes_val)

        # Evaluate using F1-score (weighted)
        f1 = f1_score(target_val, target_pred, average='weighted')
        f1_scores.append(f1)
    
    # Return the mean F1-score across all folds
    return np.mean(f1_scores)






# Create a study and optimize
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)  # Adjust the number of trials as needed

# Output the best hyperparameters
print("Best Parameters:", study.best_params)
print("Best F1-Score:", study.best_value)
print("Best Trial:", study.best_trial)









# Retrieve the best hyperparameters
best_params = study.best_params

# Train the model on the full training set
final_model = XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    eval_metric='mlogloss',
    **best_params
)

# Apply SMOTE
attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)

# Train the final model
final_model.fit(attributes_train_smote, target_train_smote)

# Evaluate on the test set
target_test_pred = final_model.predict(attributes_test)
print(classification_report(target_test, target_test_pred))





# Visualize parameter importance
optuna.visualization.plot_param_importances(study).show()

# Visualize optimization history
optuna.visualization.plot_optimization_history(study).show()

# Visualize hyperparameter values
optuna.visualization.plot_parallel_coordinate(study).show()












param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 300],
    # 'n_estimators': [100, 300, 500],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'scale_pos_weight': [1, 10, 50]
}

grid_search = GridSearchCV(
    estimator=XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', random_state=42),
    param_grid=param_grid,
    scoring='f1_weighted',  # Use an appropriate scoring metric
    cv=tscv,                # Use TimeSeriesSplit
    verbose=4,
    n_jobs=9
)

# grid_search.fit(attributes_train_smote, target_train_smote)
# print("Best Parameters:", grid_search.best_params_)

# Output best parameters and score
print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

# Evaluate on test set
best_model = grid_search.best_estimator_
test_score = best_model.score(attributes_test, target_test)
print("Test set accuracy:", test_score)
