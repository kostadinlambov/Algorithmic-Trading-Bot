import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from functools import partial

import pickle
import mlflow
import time
import xgbfir

import optuna
import optuna.visualization as vis

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV

# Scalers
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE, ADASYN

# Metrics
from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, precision_recall_curve, f1_score

# Model
from xgboost import XGBClassifier, plot_importance








attributes = pd.read_csv("data/btc_price_data_1_year_attributes.csv")
attributes.set_index('timestamp', inplace=True, drop = True)
attributes


target_data = pd.read_csv("data/btc_price_data_1_year_target.csv")
target_data.set_index('timestamp', inplace=True, drop = True)
target = target_data['target']
target





attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape














def create_oversampling_smote(attributes_train, target_train):
    """
    Apply SMOTE(Synthetic Minority Oversampling Technique) to generate synthetic examples for minority classes
    """
    smote = SMOTE(random_state=42)
    attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (SMOTE):", np.bincount(target_train))
    print("Balanced Class Distribution (SMOTE):", np.bincount(target_train_smote))

    return attributes_train_smote, target_train_smote





def create_oversampling_adasyn(attributes_train, target_train):
    """
    Apply ADASYN(Adaptive Synthetic Sampling) to generate synthetic examples for minority classes
    """
    adasyn = ADASYN(random_state=42)
    attributes_train_adasyn, target_train_adasyn = adasyn.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (ADASYN):", np.bincount(target_train))
    print("Balanced Class Distribution (ADASYN):", np.bincount(target_train_adasyn))

    return attributes_train_adasyn, target_train_adasyn





def create_oversampling(attributes_train, target_train, oversampling_type = None):
    """
    Apply 'oversampling_type' to generate synthetic examples for minority classes
    """
    if oversampling_type == 'smote':
        return create_oversampling_smote(attributes_train, target_train)
    elif oversampling_type == 'adasyn':
        return create_oversampling_adasyn(attributes_train, target_train)
    else:
        print(f"Invalid oversampling type: [{oversampling_type}]")
        return attributes_train, target_train











def plot_classification_report_multiclass(model, attributes_test_data, target_test_data): 
    # Predictions
    target_pred_data = model.predict(attributes_test_data, verbose=-1)

    # Calculate F1-score (weighted)
    f1 = f1_score(target_test_data, target_pred_data, average='macro')

    print(f"\nF1 score (macro) : [{f1}]")
    
    # Classification Report
    print("\nClassification report: ")
    print(classification_report(target_test_data, target_pred_data))


def plot_feature_importance_xgb(model):
    # The number of times a feature appears in a tree
    plot_importance(model, importance_type = "weight")
    
    # The average gain of splits which use the feature
    plot_importance(model, importance_type = "gain")
    
    # The average coverage of splits which use the feature
    plot_importance(model, importance_type = "cover")


def plot_confusion_matrix(model, attributes_test_data, target_test_data):
    # Predictions
    target_pred_data = model.predict(attributes_test_data, verbose=-1)
    
    # Confusion Matrix Computation
    cm = confusion_matrix(target_test_data, target_pred_data)
    #print("Confusion Matrix:\n", cm)
    
    # Confusion Matrix Visualization
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], 
                yticklabels=['Class 0', 'Class 1', 'Class 2'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()


def plot_classification_error(model, eval_metric='mlogloss'):
    # Retrieve evaluation results
    eval_results = model.evals_result_
    
    # Plot classification error
    plt.figure(figsize=(10, 6))
    plt.plot(eval_results['training'][eval_metric], label='Train Error')
    plt.plot(eval_results['valid_1'][eval_metric], label='Validation Error')
    plt.xlabel('Boosting Rounds')
    plt.ylabel('Classification Error')
    plt.title(f"Classification Error ({eval_metric}) During Training ")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_roc_curve_multiclass(model,target_test_data, classes):
    print(f"\nCalculate the ROC-AUC score for each class individually to see how well the model performs for the minority classes")
    
    # Binarize labels for one-vs-rest (multiclass ROC-AUC)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities

    # ROC-AUC for multiclass
    roc_auc = roc_auc_score(target_test, target_pred_prob, multi_class='ovr') # 'ovr' (one-vs-rest), 'ovo'-(one-vs-one)
    print(f"\nROC-AUC Score: {roc_auc}")
    
    # Compute ROC-AUC for each class
    roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
    print(f"ROC-AUC per class {classes}:", roc_auc_per_class)

    # Compute ROC curve and ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot ROC curves for each class
    plt.figure()
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    
    # Configure the plot
    plt.title('ROC-AUC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()


def plot_precision_recal_curve_multiclass(model,target_test_data, classes):
    # Binarize labels for one-vs-rest (multiclass precision-recall)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities
    
    # Plot precision-recall curves for each class
    for i in range(3):  # Assuming 3 classes (0, 1, 2)
        precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        plt.plot(recall, precision, label=f'Class {i}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()


def plot_feature_importance(model):
    # Feature importances
    importances = model.feature_importances_
    feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title('Feature Importance')
    plt.show()


def evaluate_model(model, attributes_test_data, target_test_data):
    classes = [0, 1, 2]
    
    plot_classification_report_multiclass(model, attributes_test_data, target_test_data)
    plot_feature_importance_xgb(model)
    plot_confusion_matrix(model, attributes_test_data, target_test_data)
    plot_classification_error(model, eval_metric='multi_logloss')
    plot_classification_error(model, eval_metric='merror')
    plot_roc_curve_multiclass(model, target_test_data, classes)
    plot_precision_recal_curve_multiclass(model, target_test_data, classes)


# evaluate_model(lgb_model, attributes_test, target_test)











initial_xgboost_model_parameters = {
    'objective': 'multi:softmax',                                  # Multiclass classification
    'num_class': 3,                                                # Number of target classes
    'eval_metric': ['mlogloss', 'merror'],                         # Evaluation metric
    'max_depth': 6,                                                # Controls model complexity
    'learning_rate': 0.1,                                          # Step size for weight updates
    'n_estimators': 300,                                           # Number of boosting rounds
    'subsample': 0.8,                                              # Fraction of data used for training
    'colsample_bytree': 0.8,                                       # Fraction of features used
    'reg_alpha': 0.5,                                                # L1 regularization
    'reg_lambda': 0.07,                                              # L2 regularization
    'min_child_weight': 2,
    #'early_stopping_rounds': 10,                                   # Stop if no improvement in 10 rounds
    'verbosity': 3,
    'n_jobs': 10,
    'random_state': 42
}














def train_model(model_params, attributes_train_data, target_train_data, attributes_test_data, target_test_data, oversampling_type = None, log_evaluation_period = 10):
    print("\nmodel_params:", model_params)
    
    # Apply oversampling to handle class imbalance in training data
    attributes_train_data_oversampling, target_train_data_oversampling = create_oversampling(
            attributes_train_data,
            target_train_data, 
            oversampling_type
    )
    
    # Use both the training data and validation data in the eval_set to monitor both training and validation performance during training
    eval_set = [
        (attributes_train_data_oversampling, target_train_data_oversampling), # Resampled training data
        (attributes_test_data, target_test_data)    # Original validation data
    ]

    # Create XGBClassifier with the params
    xgb_model = XGBClassifier(**model_params)
        
    # Train XGBoost model
    xgb_model.fit(
        attributes_train_data_oversampling,
        target_train_data_oversampling,
        eval_set = eval_set,
        verbose=False)
    
    # Evaluate the model
    # evaluate_model(lgb_model, attributes_test_data, target_test_data)
    
    return xgb_model


def fit_model_with_timeseries_split(model_params, attributes_train_data, target_train_data, attributes_test_data, target_test_data, oversampling_type = None, log_evaluation_period = 10):
    """
    Cross-Validate with TimeSeriesSplit - using TimeSeriesSplit on the training data to ensure 
    the model performs consistently across different time windows.
    """
     # Metrics collection
    scores = {
        #  The F1 score is the harmonic mean of precision and recall. The 'macro' version calculates the F1 score 
        #  for each class independently and then averages them, giving equal weight to all classes, regardless of their size
        'f1_macro_scores' : [],
        # Balanced accuracy is the average of recall obtained on each class. It accounts for class imbalance 
        # by giving equal weight to each class
        'balanced_acc_scores' : [],
        'f1_weighted_scores' : [],
        'kappa_scores' : [],
        'log_loss_scores' : [],
        'roc_auc_scores' : [],
    }

    # Use TimeSeriesSplit for time series cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes_train_data)):
        attributes_train_fold, attributes_val_fold = attributes_train_data.iloc[train_idx], attributes_train_data.iloc[val_idx]
        target_train_fold, target_val_fold = target_train_data.iloc[train_idx], target_train_data.iloc[val_idx]

        # Train the LightGB model with the current fold data
        xgb_model_split = train_model(
            model_params = model_params,
            attributes_train_data = attributes_train_fold,
            target_train_data = target_train_fold,
            attributes_test_data = attributes_test_data,
            target_test_data = target_test_data, 
            oversampling_type = oversampling_type,
            log_evaluation_period = log_evaluation_period)

        # Predictions and probabilities
        target_val_pred = xgb_model_split.predict(attributes_val_fold)
        target_val_proba = xgb_model_split.predict_proba(attributes_val_fold)
        
        # Calculate metrics for this fold
        scores['f1_macro_scores'].append(f1_score(target_val_fold, target_val_pred, average='macro'))
        scores['f1_weighted_scores'].append(f1_score(target_val_fold, target_val_pred, average='weighted'))
        scores['balanced_acc_scores'].append(balanced_accuracy_score(target_val_fold, target_val_pred))
        scores['kappa_scores'].append(cohen_kappa_score(target_val_fold, target_val_pred))
        scores['log_loss_scores'].append(log_loss(target_val_fold, target_val_proba))
        scores['roc_auc_scores'].append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))

        print(f"\nFold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))

    # Aggregate metrics across folds
    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}

    print("\nCross-Validated Metrics:")
    print(f"F1-Score (Macro): {avg_scores['f1_macro_scores']:.4f} ± {np.std(scores['f1_macro_scores']):.4f}")
    print(f"F1-Score (Weighted): {avg_scores['f1_weighted_scores']:.4f} ± {np.std(scores['f1_weighted_scores']):.4f}")
    print(f"Balanced Accuracy: {avg_scores['balanced_acc_scores']:.4f} ± {np.std(scores['balanced_acc_scores']):.4f}")
    print(f"Cohen's Kappa: {avg_scores['kappa_scores']:.4f} ± {np.std(scores['kappa_scores']):.4f}")
    print(f"Log Loss: {avg_scores['log_loss_scores']:.4f} ± {np.std(scores['log_loss_scores']):.4f}")
    print(f"ROC-AUC: {avg_scores['roc_auc_scores']:.4f} ± {np.std(scores['roc_auc_scores']):.4f}")

    return avg_scores


def objective(trial, model_type, oversampling_type = None, n_splits = 5, log_evaluation_period = 10):
    print(f"\nStart trial:[{trial.number}] for model:[{model_type}] with time series cross validation, oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")

    # Define the hyperparameters to tune
    params = {
        'objective': 'multi:softmax',                                                # Multiclass classification
        'num_class': 3,                                                              # Number of target classes
        'eval_metric': ['mlogloss', 'merror'],                                       # Evaluation metric
        'max_depth': trial.suggest_int('max_depth', 3, 10),                          # Tree depth - controls model complexity
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log = True),  # Step size for weight updates
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),                 # Number of boosting rounds
        # 'n_estimators': trial.suggest_int('n_estimators', 100, 500),               # Number of boosting rounds
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),                     # Row sampling - fraction of data used for training
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),       # Feature sampling - fraction of features used
        # 'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 50),        # Class imbalance - for  a multiclass classification  this parameter can't be applied.
        'reg_alpha': trial.suggest_float("reg_alpha", 0, 10),                        # L1 regularization
        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log = True),      # L2 regularization --> good values typically fall in [0,10]
        'min_child_weight': trial.suggest_int("min_child_weight", 1, 10),
        'early_stopping_rounds': 10,                                                  # Stop if no improvement in 10 rounds
        'n_jobs' : 10,
        'random_state': 42
    }

    
    # Train the model with the 'params'
    avg_scores = fit_model_with_timeseries_split(params, attributes_train, target_train, attributes_test, target_test, oversampling_type, log_evaluation_period)

    # Log all scores to Optuna
    trial.set_user_attr("scores", avg_scores)
    
    # Return the mean F1-macro across all folds
    return avg_scores['f1_macro_scores']



# def objective(trial, oversampling_type = None, n_splits = 5):
#     print(f"Start trial:[{trial.number}] with time series cross validation, oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")

#     # Define the hyperparameters to tune
#     params = {
#         'objective': 'multi:softmax',                                                # Multiclass classification
#         'num_class': 3,                                                              # Number of target classes
#         'eval_metric': ['mlogloss', 'merror'],                                       # Evaluation metric
#         'max_depth': trial.suggest_int('max_depth', 3, 10),                          # Tree depth - controls model complexity
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log = True),  # Step size for weight updates
#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),                 # Number of boosting rounds
#         # 'n_estimators': trial.suggest_int('n_estimators', 100, 500),               # Number of boosting rounds
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),                     # Row sampling - fraction of data used for training
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),       # Feature sampling - fraction of features used
#         # 'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 50),        # Class imbalance - for  a multiclass classification  this parameter can't be applied.
#         'reg_alpha': trial.suggest_float("reg_alpha", 0, 10),                        # L1 regularization
#         'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log = True),      # L2 regularization --> good values typically fall in [0,10]
#         'min_child_weight': trial.suggest_int("min_child_weight", 1, 10),
#         'random_state': 42,
#         'early_stopping_rounds': 10,                                                  # Stop if no improvement in 10 rounds
#         'n_jobs' : 9
#     }

#     # Use TimeSeriesSplit for time series cross-validation
#     tscv = TimeSeriesSplit(n_splits = n_splits)
    
#     f1_scores = []  # Store F1-scores for each fold

#     for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes)):
#         attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
#         target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]

#         # Apply oversampling to handle class imbalance in training data
#         attributes_train_fold, target_train_fold = create_oversampling(
#             attributes_train_fold,
#             target_train_fold, 
#             oversampling_type
#         )

#         # Create XGBClassifier with the params
#         model = XGBClassifier(**params)
        
#         # Train XGBoost model
#         model.fit(attributes_train_fold,
#                   target_train_fold,
#                   eval_set=[(attributes_val_fold, target_val_fold)],
#                   verbose=False)

#         # Predictions and probabilities
#         target_val_pred = rf_model_split.predict(attributes_val_fold)
#         target_val_proba = rf_model_split.predict_proba(attributes_val_fold)

#         # Calculate metrics for this fold
#         f1_macro_scores.append(f1_score(target_val_fold, target_val_pred, average='macro'))
#         f1_weighted_scores.append(f1_score(target_val_fold, target_val_pred, average='weighted'))
#         balanced_acc_scores.append(balanced_accuracy_score(target_val_fold, target_val_pred))
#         kappa_scores.append(cohen_kappa_score(target_val_fold, target_val_pred))
#         log_loss_scores.append(log_loss(target_val_fold, target_val_proba))
#         roc_auc_scores.append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))

#         print(f"Fold {fold+1} Classification Report")
#         print(classification_report(target_val_fold, target_val_pred))
    
#     # Return the mean F1-macro across all folds
#     return np.mean(f1_macro_scores)





# Set model type
model_type = 'XGBoost'

# Set oversampling type
oversampling_type = 'adasyn'

n_trials = 2 # default : 50
n_splits = 2 # default : 5
posfix = '_test_1'
mlflow_run_name = f"BTC_{model_type}_Optuna_{oversampling_type}_Trials_{n_trials}{posfix}"

print("Oversampling Type:", oversampling_type)
print("n_trials:", n_trials)
print("n_splits:", n_splits)
print("mlflow_run_name:", mlflow_run_name)


# model_type = 'XGBClassifier'
# n_trials = 3 # default : 50
# n_splits = 3 # default : 5
# posfix = ''
# mlflow_run_name = f"BTC_{model_type}_Optuna_{oversampling_type}-{n_trials}-Trials{posfix}"


# Create a MLFlow experiment
experiment_name = f"BTC_{model_type}_Optuna"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None: 
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)


with mlflow.start_run(run_name = mlflow_run_name):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))

    # Create a Optuna study and optimize
    study = optuna.create_study(direction='maximize')
    study.optimize(
        partial(objective, oversampling_type = oversampling_type, n_splits = 5),
        n_trials = n_trials # adjust the number of trials as needed
    )

    # Visualize optimization history
    optimization_history_plot = vis.plot_optimization_history(study)
    # Visualize parameter importance
    param_importance_plot = vis.plot_param_importances(study)
    # Visualize hyperparameter values
    hyperparameter_values_plot = vis.plot_parallel_coordinate(study)

    # Save visualizations to files
    optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Log visualization MLflow artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_F1_macro-score", study.best_value)

    # Output the best hyperparameters
    print("Best Parameters:", study.best_params)
    print("Best F1-macro-score:", study.best_value)
    print("Best Trial:", study.best_trial)

    # Train the model with evaluation metrics
    eval_set = [(attributes_train, target_train), (attributes_test, target_test)]

    # Train best model
    merged_params = initial_xgboost_model_parameters | study.best_params

    print("merged_params:", merged_params)
    
    model_xgb = XGBClassifier(**merged_params)
    model_xgb.fit(attributes_train, target_train, eval_set = eval_set)

    # Validate
    target_train_pred = model_xgb.predict(attributes_train)
    classification_train_report = classification_report(target_train, target_train_pred)

    target_test_pred = model_xgb.predict(attributes_test)
    classification_test_report = classification_report(target_test, target_test_pred)

    print(f"classification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
    print(f"classification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)

    # Log metrics
    mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", model_xgb.score(attributes_train, target_train))
    mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", model_xgb.score(attributes_test, target_test))

    with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_train_report)
    with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_test_report)
        
    pickle.dump(model_xgb, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))

    # Log artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
    
    classes = [0, 1, 2]
    evaluate_model(model_xgb, attributes_test, target_test, classes)





param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 300],
    # 'n_estimators': [100, 300, 500],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'scale_pos_weight': [1, 10, 50]
}

# Use TimeSeriesSplit for time series cross-validation
tscv = TimeSeriesSplit(n_splits = n_splits)

grid_search = GridSearchCV(
    estimator=XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', random_state=42),
    param_grid=param_grid,
    scoring='f1_weighted',  # Use an appropriate scoring metric
    cv=tscv,                # Use TimeSeriesSplit
    verbose=4,
    n_jobs=9
)

# grid_search.fit(attributes_train_smote, target_train_smote)
# print("Best Parameters:", grid_search.best_params_)

# Output best parameters and score
# print("Best parameters:", grid_search.best_params_)
# print("Best accuracy:", grid_search.best_score_)

# Evaluate on test set
# best_model = grid_search.best_estimator_
# test_score = best_model.score(attributes_test, target_test)
# print("Test set accuracy:", test_score)








def print_classification_report_multiclass(model, attributes_test_data, target_test_data): 
    # Predictions
    target_pred_data = model.predict(attributes_test_data)

    # Calculate F1-score (weighted)
    f1 = f1_score(target_test_data, target_pred_data, average='weighted')

    print(f"F1 score (weighted) : [{f1}]")
    
    # Classification Report
    print("Classification report: ")
    print(classification_report(target_test_data, target_pred_data))


def plot_classification_error(model, eval_metric='mlogloss'):
    # Retrieve evaluation results
    eval_results = model.evals_result()
    
    # Plot classification error
    plt.figure(figsize=(10, 6))
    plt.plot(eval_results['validation_0'][eval_metric], label='Train Error')
    plt.plot(eval_results['validation_1'][eval_metric], label='Validation Error')
    plt.xlabel('Boosting Rounds')
    plt.ylabel('Classification Error')
    plt.title(f"Classification Error ({eval_metric}) During Training ")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_roc_curve_multiclass(model,target_test_data, classes):
    print(f"Calculate the ROC-AUC score for each class individually to understand how well the model performs for the minority classes")
    
    # Binarize labels for one-vs-rest (multiclass ROC-AUC)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities

    # ROC-AUC for multiclass
    roc_auc = roc_auc_score(target_test, target_pred_prob, multi_class='ovr') # 'ovr' (one-vs-rest), 'ovo'-(one-vs-one)
    print(f"ROC-AUC Score: {roc_auc}")
    
    # Compute ROC-AUC for each class
    roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
    print(f"ROC-AUC per class {classes}:", roc_auc_per_class)

    # Compute ROC curve and ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot ROC curves for each class
    plt.figure()
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    
    # Configure the plot
    plt.title('ROC-AUC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()


def plot_precision_recal_curve_multiclass(model,target_test_data, classes):
    # Binarize labels for one-vs-rest (multiclass precision-recall)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities
    
    # Plot precision-recall curves for each class
    for i in range(3):  # Assuming 3 classes (0, 1, 2)
        precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        plt.plot(recall, precision, label=f'Class {i}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()


def plot_feature_importance_xgb(model):
    plot_importance(model)


def evaluate_model(model, attributes_test_data, target_test_data, classes):
    print_classification_report_multiclass(model, attributes_test_data, target_test_data)
    plot_classification_error(model, eval_metric='mlogloss')
    plot_classification_error(model, eval_metric='merror')
    plot_roc_curve_multiclass(model, target_test_data, classes)
    plot_precision_recal_curve_multiclass(model, target_test_data, classes)
    plot_feature_importance_xgb(model)


classes = [0, 1, 2]
evaluate_model(model_xgb, attributes_test, target_test, classes)





def run_time_series_cross_validation(model, attributes_data, oversampling_type = None, n_splits = 5):
    """
    Runs a cross validation for time series data. The 'attributes_data' is splitted with a 'TimeSeriesSplit' in 'n_splits'-folds
    
    Parameters:
    ----------
    model : the xgb model that will be used for the cross validation
    
    attributes_data: attributes data for the cross validation
    
    n_splits : number of splis the data for the cross validation
    
    oversampling_type : applies oversampling to generate synthetic examples for minority classes. Possible values: 'adasyn' or 'smote'
    """ 
    
    print(f"Start time series cross validation with oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")
    
    # Use TimeSeriesSplit for cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes_data)):
        attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
        target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]
        
        attributes_train_fold, target_train_fold = create_oversampling(
            attributes_train_fold,
            target_train_fold, 
            oversampling_type = oversampling_type
        )
        
        # Train model 
        model.fit(attributes_train_fold,
                  target_train_fold,  
                  eval_set=[(attributes_val_fold, target_val_fold)],
                  verbose=False)
    
        # Validate
        target_val_pred = model.predict(attributes_val_fold)
        
        print(f"Fold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))


run_time_series_cross_validation(model_xgb, attributes, oversampling_type, n_splits = 5)



