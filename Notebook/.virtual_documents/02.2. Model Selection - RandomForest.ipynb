import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from functools import partial

import pickle
import mlflow
import time

import optuna
import optuna.visualization as vis

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV

# Scalers
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE, ADASYN

# Metrics
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, auc, roc_curve, precision_recall_curve, f1_score, confusion_matrix, balanced_accuracy_score, cohen_kappa_score, log_loss 

# Model
from sklearn.ensemble import RandomForestClassifier








attributes = pd.read_csv("data/btc_price_data_1_year_attributes.csv")
attributes.set_index('timestamp', inplace=True, drop = True)
attributes


target_data = pd.read_csv("data/btc_price_data_1_year_target.csv")
target_data.set_index('timestamp', inplace=True, drop = True)
target = target_data['target']
target





attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape














def create_oversampling_smote(attributes_train, target_train):
    """
    Apply SMOTE(Synthetic Minority Oversampling Technique) to generate synthetic examples for minority classes
    """
    smote = SMOTE(random_state=42)
    attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (SMOTE):", np.bincount(target_train))
    print("Balanced Class Distribution (SMOTE):", np.bincount(target_train_smote))

    return attributes_train_smote, target_train_smote





def create_oversampling_adasyn(attributes_train, target_train):
    """
    Apply ADASYN(Adaptive Synthetic Sampling) to generate synthetic examples for minority classes
    """
    adasyn = ADASYN(random_state=42)
    attributes_train_adasyn, target_train_adasyn = adasyn.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (ADASYN):", np.bincount(target_train))
    print("Balanced Class Distribution (ADASYN):", np.bincount(target_train_adasyn))

    return attributes_train_adasyn, target_train_adasyn





def create_oversampling(attributes_train, target_train, oversampling_type = None):
    """
    Apply 'oversampling_type' to generate synthetic examples for minority classes
    """
    if oversampling_type == 'smote':
        return create_oversampling_smote(attributes_train, target_train)
    elif oversampling_type == 'adasyn':
        return create_oversampling_adasyn(attributes_train, target_train)
    else:
        print(f"Invalid oversampling type: [{oversampling_type}]")
        return attributes_train, target_train














def plot_feature_importance(model, importances, feature_names, title = 'Feature Importance'):
    # Feature importances
    # importances = model.feature_importances_
    # feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title(title)
    plt.show()


def remove_low_important_features(rf_model, attributes, attributes_type):
    importances = rf_model.feature_importances_
    feature_names = attributes.columns
    
    # Plot the importances
    plot_feature_importance(rf_model, importances, feature_names, title = f'Feature Importance({attributes_type}) before drop')
    
    # Feature importance
    feature_importances = pd.DataFrame({
        'Feature': attributes.columns,
        'Importance': rf_model.feature_importances_
    }).sort_values(by="Importance", ascending=False)
    
    # Drop features with low importance
    low_importance_features = feature_importances[feature_importances['Importance'] < 0.01]['Feature']
    attributes_reg = attributes.drop(columns=low_importance_features)

    print('attributes.shape before drop: ' , attributes.shape)
    print('attributes.shape after the drop: ', attributes_reg.shape)

    # Plot the importances after the drop
    # plot_feature_importance(rf_model, high_importance_features.Importance, high_importance_features.Feature, title = 'Feature Importance after the drop')

    return attributes_reg








def plot_classification_report_multiclass(model, attributes_test_data, target_test_data): 
    # Predictions
    target_pred_data = model.predict(attributes_test_data)

    # Calculate F1-score (weighted)
    f1 = f1_score(target_test_data, target_pred_data, average='macro')

    print(f"\nF1 score (macro) : [{f1}]")
    
    # Classification Report
    print("\nClassification report: ")
    print(classification_report(target_test_data, target_pred_data))


def plot_feature_importance(model):
    # Feature importances
    importances = model.feature_importances_
    feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title('Feature Importance')
    plt.show()


def plot_confusion_matrix(model, attributes_test_data, target_test_data):
    # Predictions
    target_pred_data = model.predict(attributes_test_data)
    
    # Confusion Matrix Computation
    cm = confusion_matrix(target_test_data, target_pred_data)
    #print("Confusion Matrix:\n", cm)
    
    # Confusion Matrix Visualization
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], 
                yticklabels=['Class 0', 'Class 1', 'Class 2'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()


def plot_classification_error(model, eval_metric='mlogloss'):
    # Retrieve evaluation results
    eval_results = model.evals_result()
    
    # Plot classification error
    plt.figure(figsize=(10, 6))
    plt.plot(eval_results['validation_0'][eval_metric], label='Train Error')
    plt.plot(eval_results['validation_1'][eval_metric], label='Validation Error')
    plt.xlabel('Boosting Rounds')
    plt.ylabel('Classification Error')
    plt.title(f"Classification Error ({eval_metric}) During Training ")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_roc_curve_multiclass(model,target_test_data, classes):
    print(f"\nCalculate the ROC-AUC score for each class individually to see how well the model performs for the minority classes")
    
    # Binarize labels for one-vs-rest (multiclass ROC-AUC)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities

    # ROC-AUC for multiclass
    roc_auc = roc_auc_score(target_test, target_pred_prob, multi_class='ovr') # 'ovr' (one-vs-rest), 'ovo'-(one-vs-one)
    print(f"\nROC-AUC Score: {roc_auc}")
    
    # Compute ROC-AUC for each class
    roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
    print(f"ROC-AUC per class {classes}:", roc_auc_per_class)

    # Compute ROC curve and ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot ROC curves for each class
    plt.figure()
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    
    # Configure the plot
    plt.title('ROC-AUC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()


def plot_precision_recal_curve_multiclass(model,target_test_data, classes):
    # Binarize labels for one-vs-rest (multiclass precision-recall)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities
    
    # Plot precision-recall curves for each class
    for i in range(3):  # Assuming 3 classes (0, 1, 2)
        precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        plt.plot(recall, precision, label=f'Class {i}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()


def plot_feature_importance(model):
    # Feature importances
    importances = model.feature_importances_
    feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title('Feature Importance')
    plt.show()


def evaluate_model(model, attributes_test_data, target_test_data):
    classes = [0, 1, 2]
    
    plot_classification_report_multiclass(model, attributes_test_data, target_test_data)
    plot_feature_importance(model)
    plot_confusion_matrix(model, attributes_test_data, target_test_data)
    # plot_classification_error(model, eval_metric='mlogloss')
    # plot_classification_error(model, eval_metric='merror')
    plot_roc_curve_multiclass(model, target_test_data, classes)
    plot_precision_recal_curve_multiclass(model, target_test_data, classes)


# evaluate_model(lgb_model, attributes_test, target_test)








initial_forest_model_parameters = {
    'n_estimators': 300,                                         
    'max_depth': 10,                                             
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'bootstrap': True,
    'class_weight': {0: 1, 1: 50, 2: 50},
    'random_state': 42,
    'verbose': 1,
    'n_jobs': 9
}








def train_model(model_params, attributes_train_data, target_train_data, attributes_test_data, target_test_data, oversampling_type = None):
    print("\nmodel_params:", model_params)
    
    # Apply oversampling to handle class imbalance in training data
    attributes_train_data_oversampling, target_train_data_oversampling = create_oversampling(
            attributes_train_data,
            target_train_data, 
            oversampling_type
    )
    
    # Use both the training data and validation data in the eval_set to monitor both training and validation performance during training
    eval_set = [
        (attributes_train_data_oversampling, target_train_data_oversampling), # Resampled training data
        (attributes_test_data, target_test_data)    # Original validation data
    ]

    # Create RandomForestClassifier with the params
    rf_model = RandomForestClassifier(**model_params)
    
    # Train XGBoost model
    rf_model.fit(
        attributes_train_data_oversampling,
        target_train_data_oversampling)
        # eval_set = eval_set)
    
    # Evaluate the model
    # evaluate_model(lgb_model, attributes_test_data, target_test_data)
    
    return rf_model


def fit_model_with_timeseries_split(model_params, attributes_train_data, target_train_data, attributes_test_data, target_test_data, oversampling_type = None):
    """
    Cross-Validate with TimeSeriesSplit - using TimeSeriesSplit on the training data to ensure 
    the model performs consistently across different time windows.
    """
     # Metrics collection
    scores = {
        #  The F1 score is the harmonic mean of precision and recall. The 'macro' version calculates the F1 score 
        #  for each class independently and then averages them, giving equal weight to all classes, regardless of their size
        'f1_macro_scores' : [],
        # Balanced accuracy is the average of recall obtained on each class. It accounts for class imbalance 
        # by giving equal weight to each class
        'balanced_acc_scores' : [],
        'f1_weighted_scores' : [],
        'kappa_scores' : [],
        'log_loss_scores' : [],
        'roc_auc_scores' : [],
    }

    # Use TimeSeriesSplit for time series cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes_train_data)):
        attributes_train_fold, attributes_val_fold = attributes_train_data.iloc[train_idx], attributes_train_data.iloc[val_idx]
        target_train_fold, target_val_fold = target_train_data.iloc[train_idx], target_train_data.iloc[val_idx]

        # Train the LightGB model with the current fold data
        rf_model_split = train_model(
            model_params = model_params,
            attributes_train_data = attributes_train_fold,
            target_train_data = target_train_fold,
            attributes_test_data = attributes_test_data,
            target_test_data = target_test_data, 
            oversampling_type = oversampling_type)

        # Predictions and probabilities
        target_val_pred = rf_model_split.predict(attributes_val_fold)
        target_val_proba = rf_model_split.predict_proba(attributes_val_fold)
        
        # Calculate metrics for this fold
        scores['f1_macro_scores'].append(f1_score(target_val_fold, target_val_pred, average='macro'))
        scores['f1_weighted_scores'].append(f1_score(target_val_fold, target_val_pred, average='weighted'))
        scores['balanced_acc_scores'].append(balanced_accuracy_score(target_val_fold, target_val_pred))
        scores['kappa_scores'].append(cohen_kappa_score(target_val_fold, target_val_pred))
        scores['log_loss_scores'].append(log_loss(target_val_fold, target_val_proba))
        scores['roc_auc_scores'].append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))

        print(f"\nFold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))

    # Aggregate metrics across folds
    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}

    print("\nCross-Validated Metrics:")
    print(f"F1-Score (Macro): {avg_scores['f1_macro_scores']:.4f} ± {np.std(scores['f1_macro_scores']):.4f}")
    print(f"F1-Score (Weighted): {avg_scores['f1_weighted_scores']:.4f} ± {np.std(scores['f1_weighted_scores']):.4f}")
    print(f"Balanced Accuracy: {avg_scores['balanced_acc_scores']:.4f} ± {np.std(scores['balanced_acc_scores']):.4f}")
    print(f"Cohen's Kappa: {avg_scores['kappa_scores']:.4f} ± {np.std(scores['kappa_scores']):.4f}")
    print(f"Log Loss: {avg_scores['log_loss_scores']:.4f} ± {np.std(scores['log_loss_scores']):.4f}")
    print(f"ROC-AUC: {avg_scores['roc_auc_scores']:.4f} ± {np.std(scores['roc_auc_scores']):.4f}")

    return avg_scores


def objective(trial, model_type, oversampling_type = None, n_splits = 5):
    print(f"\nStart trial:[{trial.number}] for model:[{model_type}] with time series cross validation, oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")

    # Define the hyperparameters to tune
    params = {
        # 'n_estimators': trial.suggest_int("n_estimators", 5, 25),     
        'n_estimators': trial.suggest_int("n_estimators", 50, 300),     
        'max_depth': trial.suggest_int("max_depth", 3, 10),   # Limits the depth of individual trees. Restricting tree depth prevents overfitting by forcing trees to be smaller and less complex.                                       
        'min_samples_split': trial.suggest_int("min_samples_split", 5, 10), # The minimum number of samples required to split an internal node. Increasing this value reduces tree branching
        'min_samples_leaf':  trial.suggest_int("min_samples_leaf", 3, 8), # The minimum number of samples required to be at a leaf node. Larger values create shallower trees and reduce overfitting
        'max_features': 'sqrt', # The number of features to consider when looking for the best split. Lower values add randomness to the splits, making the model less prone to overfitting
        'bootstrap': True,
        # 'class_weight': {0: 1, 1: 50, 2: 50},
        'random_state': 42,
        'verbose': 1,
        'n_jobs': -1,
        
        'class_weight': {    # Custom class weights. Classes 1 and 2 are very unbalanced. We therefore use fairly large values for the weights.
            0: 1,
            1: trial.suggest_int("class_weight_1", 30, 100),
            2: trial.suggest_int("class_weight_2", 30, 100)
        }
    }
    
    # Train the model with the 'params'
    avg_scores = fit_model_with_timeseries_split(params, attributes_train, target_train, attributes_test, target_test, oversampling_type)

    # Log all scores to Optuna
    trial.set_user_attr("scores", avg_scores)
    
    # Return the mean F1-macro across all folds
    return avg_scores['f1_macro_scores']






# Set model type
model_type = 'RandomForestClassifier'

# Set oversampling type
oversampling_type = 'adasyn'

n_trials = 50 # default : 50
n_splits = 5 # default : 5
posfix = '_test_1'
mlflow_run_name = f"BTC_{model_type}_Optuna_{oversampling_type}_Trials_{n_trials}{posfix}"

print("Oversampling Type:", oversampling_type)
print("n_trials:", n_trials)
print("n_splits:", n_splits)
print("mlflow_run_name:", mlflow_run_name)


# Create a MLFlow experiment
experiment_name = f"BTC_{model_type}_Optuna"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None:
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)


def start_mlflow_run(run_name, use_optuna = True, n_splits = 5, initial_forest_model_parameters = initial_forest_model_parameters):
    with mlflow.start_run(run_name = run_name):
        mlflow.log_param("train size", len(attributes_train))
        mlflow.log_param("test size", len(attributes_test))

        merged_params = initial_forest_model_parameters
    
        if use_optuna:
            # Create a Optuna study and optimize
            study = optuna.create_study(direction = 'maximize')
            study.optimize(
                partial(objective, model_type = model_type, oversampling_type = oversampling_type, n_splits = n_splits),
                n_trials = n_trials
            )
    
            mlflow.log_metric("best_F1-macro_score_fine_tuning", study.best_value)
            
            # Visualize optimization history
            optimization_history_plot = vis.plot_optimization_history(study)
            # Visualize parameter importance
            param_importance_plot = vis.plot_param_importances(study)
            # Visualize hyperparameter values
            hyperparameter_values_plot = vis.plot_parallel_coordinate(study)
        
            # Save visualizations to files
            optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
            param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
            hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")
        
            # Log visualization MLflow artifacts
            mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
            mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
            mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

            ################################################################################
            # log best params
            ################################################################################
            # Get the best trial
            best_trial = study.best_trial
        
            # Output the best hyperparameters
            print("\n\nBest Parameters:", study.best_params)
            print("\nBest F1-macro-score-fine-tuning:", study.best_value)
            print("\nBest Trial:", best_trial)
        
            # Access the parameters of the best trial
            print("\n\nBest Trial Parameters:")
            print(f"Trial {best_trial.number}:")
            print(f"Parameters: {best_trial.params}")
            
           # Access the scores stored in user attributes
            print("\nBest Trial Scores:")
            for metric, value in best_trial.user_attrs['scores'].items():
                print(f"{metric}: {value:.4f}")
                mlflow.log_metric(metric, value)
        
            ################################################################################
            # END: log best params
            ################################################################################
            
            # Get the best hyper params from the best model
            merged_params = initial_lgbm_model_parameters | study.best_params
            del merged_params['class_weight_1']
            del merged_params['class_weight_2']

            print("\nmerged_params:", merged_params)
        
        mlflow.log_params(merged_params)
    
        # # Removing irrelevant features (feature selection)
        # attributes_train_reg = remove_low_important_features(rf_model, attributes_train, attributes_type = 'train')
        # attributes_test_reg = remove_low_important_features(rf_model, attributes_test, attributes_type = 'test')
    
        # Train the model with the best params and TimeSeriesSplit
        avg_scores = fit_model_with_timeseries_split(merged_params, attributes_train, target_train, attributes_test, target_test, oversampling_type)
    
        print(f"\nF1-Score(Macro) with Best Params and TimeSplit: {avg_scores['f1_macro_scores']:.4f}")
        # Access the scores stored in user attributes
        print("\nScores with Best Params and TimeSplit:")
        for metric, value in avg_scores.items():
            print(f"{metric}: {value:.4f}")
            mlflow.log_metric(metric, value)

        # Final training the model with the best params without timesplit
        rf_model = train_model(merged_params, attributes_train, target_train, attributes_test, target_test, oversampling_type)
        
        # Validate
        target_train_pred = rf_model.predict(attributes_train)
        target_test_pred = rf_model.predict(attributes_test)
        
        # Validate
        classification_train_report = classification_report(target_train, target_train_pred)
        classification_test_report = classification_report(target_test, target_test_pred)
       
        print(f"\nclassification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
        print(f"\nclassification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)
    
        # F1-macro score after the final model tarining on all available data (training + test)
        f1_macro_score = f1_score(target_test, target_test_pred, average='macro')
        print(f"F1-Score (Macro) final training: {f1_macro_score:.4f}")
        
        mlflow.log_metric("F1-macro-score-final", f1_macro_score)
    
        # Log metrics
        mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", rf_model.score(attributes_train, target_train))
        mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", rf_model.score(attributes_test, target_test))
    
        with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
            f.write(classification_train_report)
        with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
            f.write(classification_test_report)
            
        pickle.dump(rf_model, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))
    
        # Log artifacts
        mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
        mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
        mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
        
        evaluate_model(rf_model, attributes_test, target_test)


best_forest_model_params_adasyn = {
    'n_estimators': 193,                                         
    'max_depth': 10,                                             
    'min_samples_split':5,
    'min_samples_leaf': 3,
    'bootstrap': True,
    'class_weight': {0: 1, 1: 92, 2: 80},
    'random_state': 42,
    'verbose': 1,
    'n_jobs': 10
}

# Set oversampling_type to 'adasyn'
oversampling_type = 'adasyn'

n_splits = 2

# Set mlflow_run_name
posfix = '_train_best_model'
mlflow_run_name = f"BTC_{model_type}_{oversampling_type}{posfix}"
print(mlflow_run_name)


# Train with best params and 'adasyn'
start_mlflow_run(mlflow_run_name, use_optuna = False, n_splits = n_splits, initial_forest_model_parameters = best_forest_model_params_adasyn)


best_forest_model_params_smote = {
    'n_estimators': 107,                                         
    'max_depth': 10,                                             
    'min_samples_split':8,
    'min_samples_leaf': 7,
    'bootstrap': True,
    'class_weight': {0: 1, 1: 92, 2: 80},
    'random_state': 42,
    'verbose': 1,
    'n_jobs': 10
}

# Set oversampling_type to 'smote'
oversampling_type = 'smote'

n_splits = 2

# Set mlflow_run_name
posfix = '_train_best_model'
mlflow_run_name = f"BTC_{model_type}_{oversampling_type}{posfix}"
print(mlflow_run_name)


# Train with best params and 'smote'
start_mlflow_run(mlflow_run_name, use_optuna = False, n_splits = n_splits, initial_forest_model_parameters = best_forest_model_params_smote)








from sklearn.pipeline import Pipeline

X = attributes
y = target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = False)

# Define pipeline with placeholder for scaler and model
pipeline = Pipeline([
    ('scaler', None),  # Placeholder for scaler
    ('rf', RandomForestClassifier(random_state=42))
])

# Define parameter grid for grid search
param_grid = {
    'scaler': [None, MinMaxScaler(), StandardScaler()],  # Test with and without scaling
    'rf__n_estimators': [100, 300],                # Number of trees
    # 'rf__n_estimators': [100, 300, 500],                # Number of trees
    'rf__max_depth': [None, 10, 20, 30],                # Tree depth
    'rf__min_samples_split': [2, 5, 10],                # Min samples required to split
    'rf__min_samples_leaf': [1, 2, 4],                  # Min samples required at leaf node
    'rf__max_features': ['sqrt', 'log2', None],         # Max features considered for split
    'rf__bootstrap': [True, False]                      # Use bootstrap sampling or not
}

# Perform grid search
# grid_search = GridSearchCV(
#     pipeline,
#     param_grid,
#     cv=2,                # 3-fold cross-validation
#     # cv=3,                # 3-fold cross-validation
#     scoring='accuracy',  # Optimize for accuracy
#     verbose=2,
#     n_jobs=-9          
#     # n_jobs=-1            # Use all available CPU cores
# )

# # Fit grid search
# grid_search.fit(X_train, y_train)

# # Output best parameters and score
# print("Best parameters:", grid_search.best_params_)
# print("Best accuracy:", grid_search.best_score_)

# # Evaluate on test set
# best_model = grid_search.best_estimator_
# test_score = best_model.score(X_test, y_test)
# print("Test set accuracy:", test_score)

