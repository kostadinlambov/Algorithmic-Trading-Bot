import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from functools import partial

import pickle
import mlflow
import time

import optuna
import optuna.visualization as vis

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV

# Scalers
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE, ADASYN

# Metrics
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, auc, roc_curve, precision_recall_curve, f1_score, confusion_matrix, balanced_accuracy_score, cohen_kappa_score, log_loss 

# Model
from sklearn.ensemble import RandomForestClassifier








attributes = pd.read_csv("data/btc_price_data_1_year_attributes.csv")
attributes.set_index('timestamp', inplace=True, drop = True)
attributes


target_data = pd.read_csv("data/btc_price_data_1_year_target.csv")
target_data.set_index('timestamp', inplace=True, drop = True)
target = target_data['target']
target





attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape














def create_oversampling_smote(attributes_train, target_train):
    """
    Apply SMOTE(Synthetic Minority Oversampling Technique) to generate synthetic examples for minority classes
    """
    smote = SMOTE(random_state=42)
    attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (SMOTE):", np.bincount(target_train))
    print("Balanced Class Distribution (SMOTE):", np.bincount(target_train_smote))

    return attributes_train_smote, target_train_smote





def create_oversampling_adasyn(attributes_train, target_train):
    """
    Apply ADASYN(Adaptive Synthetic Sampling) to generate synthetic examples for minority classes
    """
    adasyn = ADASYN(random_state=42)
    attributes_train_adasyn, target_train_adasyn = adasyn.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (ADASYN):", np.bincount(target_train))
    print("Balanced Class Distribution (ADASYN):", np.bincount(target_train_adasyn))

    return attributes_train_adasyn, target_train_adasyn





# Set oversampling type
oversampling_type = 'smote'


def create_oversampling(attributes_train, target_train, oversampling_type = None):
    """
    Apply 'oversampling_type' to generate synthetic examples for minority classes
    """
    if oversampling_type == 'smote':
        return create_oversampling_smote(attributes_train, target_train)
    elif oversampling_type == 'adasyn':
        return create_oversampling_adasyn(attributes_train, target_train)
    else:
        print(f"Invalid oversampling type: [{oversampling_type}]")
        return attributes_train, target_train














def plot_feature_importance(model, importances, feature_names, title = 'Feature Importance'):
    # Feature importances
    # importances = model.feature_importances_
    # feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title(title)
    plt.show()


def remove_low_important_features(rf_model, attributes, attributes_type):
    importances = rf_model.feature_importances_
    feature_names = attributes.columns
    
    # Plot the importances
    plot_feature_importance(rf_model, importances, feature_names, title = f'Feature Importance({attributes_type}) before drop')
    
    # Feature importance
    feature_importances = pd.DataFrame({
        'Feature': attributes.columns,
        'Importance': rf_model.feature_importances_
    }).sort_values(by="Importance", ascending=False)
    
    # Drop features with low importance
    low_importance_features = feature_importances[feature_importances['Importance'] < 0.01]['Feature']
    attributes_reg = attributes.drop(columns=low_importance_features)

    print('attributes.shape before drop: ' , attributes.shape)
    print('attributes.shape after the drop: ', attributes_reg.shape)

    # Plot the importances after the drop
    # plot_feature_importance(rf_model, high_importance_features.Importance, high_importance_features.Feature, title = 'Feature Importance after the drop')

    return attributes_reg








initial_forest_model_parameters = {
    'n_estimators': 300,                                         
    'max_depth': 10,                                             
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'bootstrap': True,
    'class_weight': {0: 1, 1: 50, 2: 50},
    'random_state': 42,
    'verbose': 1,
    'n_jobs': 9
}

# 'class_weight': 'balanced',








def objective(trial, model_type, oversampling_type = None, n_splits = 5):
    print(f"Start trial:[{trial.number}] for model:[{model_type}] with time series cross validation, oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")

    # Define the hyperparameters to tune
    params = {
        # 'n_estimators': trial.suggest_int("n_estimators", 5, 25),     
        'n_estimators': trial.suggest_int("n_estimators", 50, 300),     
        'max_depth': trial.suggest_int("max_depth", 3, 10),   # Limits the depth of individual trees. Restricting tree depth prevents overfitting by forcing trees to be smaller and less complex.                                       
        'min_samples_split': trial.suggest_int("min_samples_split", 5, 10), # The minimum number of samples required to split an internal node. Increasing this value reduces tree branching
        'min_samples_leaf':  trial.suggest_int("min_samples_leaf", 3, 8), # The minimum number of samples required to be at a leaf node. Larger values create shallower trees and reduce overfitting
        'max_features': 'sqrt', # The number of features to consider when looking for the best split. Lower values add randomness to the splits, making the model less prone to overfitting
        'bootstrap': True,
        # 'class_weight': {0: 1, 1: 50, 2: 50},
        'random_state': 42,
        'verbose': 1,
        'n_jobs': -1,
        
        'class_weight': {    # Custom class weights. Classes 1 and 2 are very unbalanced. We therefore use fairly large values for the weights.
            0: 1,
            1: trial.suggest_int("class_weight_1", 30, 100),
            2: trial.suggest_int("class_weight_2", 30, 100)
        }
    }
    
    # Metrics collection
    scores = {
        #  The F1 score is the harmonic mean of precision and recall. The 'macro' version calculates the F1 score 
        #  for each class independently and then averages them, giving equal weight to all classes, regardless of their size
        'f1_macro_scores' : [],
        # Balanced accuracy is the average of recall obtained on each class. It accounts for class imbalance 
        # by giving equal weight to each class
        'balanced_acc_scores' : [],
        'f1_weighted_scores' : [],
        'kappa_scores' : [],
        'log_loss_scores' : [],
        'roc_auc_scores' : [],
    }

    # Use TimeSeriesSplit for time series cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes)):
        attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
        target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]

        # Apply oversampling to handle class imbalance in training data
        attributes_train_fold, target_train_fold = create_oversampling(
            attributes_train_fold,
            target_train_fold, 
            oversampling_type
        )
        
        # Train RandomForestClassifier model
        rf_model_split = RandomForestClassifier(**params)
        rf_model_split.fit(attributes_train_fold,target_train_fold)

        # Predictions and probabilities
        target_val_pred = rf_model_split.predict(attributes_val_fold)
        target_val_proba = rf_model_split.predict_proba(attributes_val_fold)

        # Calculate metrics for this fold
        scores['f1_macro_scores'].append(f1_score(target_val_fold, target_val_pred, average='macro'))
        scores['f1_weighted_scores'].append(f1_score(target_val_fold, target_val_pred, average='weighted'))
        scores['balanced_acc_scores'].append(balanced_accuracy_score(target_val_fold, target_val_pred))
        scores['kappa_scores'].append(cohen_kappa_score(target_val_fold, target_val_pred))
        scores['log_loss_scores'].append(log_loss(target_val_fold, target_val_proba))
        scores['roc_auc_scores'].append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))

        print(f"Fold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))

    # Aggregate metrics across folds
    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}

    print("\nCross-Validated Metrics:")
    print(f"F1-Score (Macro): {avg_scores['f1_macro_scores']:.4f} ± {np.std(scores['f1_macro_scores']):.4f}")
    print(f"F1-Score (Weighted): {avg_scores['f1_weighted_scores']:.4f} ± {np.std(scores['f1_weighted_scores']):.4f}")
    print(f"Balanced Accuracy: {avg_scores['balanced_acc_scores']:.4f} ± {np.std(scores['balanced_acc_scores']):.4f}")
    print(f"Cohen's Kappa: {avg_scores['kappa_scores']:.4f} ± {np.std(scores['kappa_scores']):.4f}")
    print(f"Log Loss: {avg_scores['log_loss_scores']:.4f} ± {np.std(scores['log_loss_scores']):.4f}")
    print(f"ROC-AUC: {avg_scores['roc_auc_scores']:.4f} ± {np.std(scores['roc_auc_scores']):.4f}")

    # print(f"F1-Score (Macro): {np.mean(f1_macro_scores):.4f} ± {np.std(f1_macro_scores):.4f}")
    # print(f"F1-Score (Weighted): {np.mean(f1_weighted_scores):.4f} ± {np.std(f1_weighted_scores):.4f}")
    # print(f"Balanced Accuracy: {np.mean(balanced_acc_scores):.4f} ± {np.std(balanced_acc_scores):.4f}")
    # print(f"Cohen's Kappa: {np.mean(kappa_scores):.4f} ± {np.std(kappa_scores):.4f}")
    # print(f"Log Loss: {np.mean(log_loss_scores):.4f} ± {np.std(log_loss_scores):.4f}")
    # print(f"ROC-AUC: {np.mean(roc_auc_scores):.4f} ± {np.std(roc_auc_scores):.4f}")

    # # Aggregate scores across folds
    # print("\nCross-Validated Metrics:")
    # for metric, score in avg_scores.items():
    #     print(f"{metric}: {score:.4f}")

    # Log all scores to Optuna
    trial.set_user_attr("scores", avg_scores)
    
    # Return the mean F1-macro across all folds
    return np.mean(f1_macro_scores)






model_type = 'RandomForestClassifier'
n_trials = 50 # default : 50
n_splits = 5 # default : 5
posfix = '_test_3'
mlflow_run_name = f"BTC_{model_type}_Optuna_{oversampling_type}_Trials_{n_trials}{posfix}"

print("Oversampling Type:", oversampling_type)
print("n_trials:", n_trials)
print("n_splits:", n_splits)
print("mlflow_run_name:", mlflow_run_name)


# Create a MLFlow experiment
experiment_name = f"BTC_{model_type}_Optuna"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None:
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)


# def merge_params(initial_forest_model_parameters, best_params):
#     # Remove class_weight keys
#     class_weight_1 = best_params.pop['class_weight_1']
#     class_weight_2 = best_params.pop['class_weight_2']

#     # Add the weight keys to the 'initila_params'
#     initila_params['class_weight'] = {0: 1, 1: class_weight_1, 2: class_weight_2}

#     # Merge the two dictionaries
#     merged_params = initial_forest_model_parameters | study.best_params
#     return merged_params


with mlflow.start_run(run_name = mlflow_run_name):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))

    # Create a Optuna study and optimize
    study = optuna.create_study(direction='maximize')
    study.optimize(
        partial(objective, model_type = model_type, oversampling_type = oversampling_type, n_splits = n_splits),
        n_trials = n_trials
    )

    # Visualize optimization history
    optimization_history_plot = vis.plot_optimization_history(study)
    # Visualize parameter importance
    param_importance_plot = vis.plot_param_importances(study)
    # Visualize hyperparameter values
    hyperparameter_values_plot = vis.plot_parallel_coordinate(study)

    # Save visualizations to files
    optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Log visualization MLflow artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    ################################################################################
    # TODO: fix logs for params
    ################################################################################
    # Get the best trial
    best_trial = study.best_trial

    # Output the best hyperparameters
    print("Best Parameters:", study.best_params)
    print("Best F1-macro-score-fine-tuning:", study.best_value)
    print("Best Trial:", best_trial)

    # Access the parameters of the best trial
    print("Best Trial Parameters:")
    print(f"Trial {best_trial.number}:")
    print(f"Parameters: {best_trial.params}")
    
   # Access the scores stored in user attributes
    print("\nBest Trial Scores:")
    for metric, value in best_trial.user_attrs['scores'].items():
        print(f"{metric}: {value:.4f}")
        mlflow.log_metric(metric, value)

    # print("\nRecommended hyperparameters for subsequent use:")
    # print("rf_model = RandomForestClassifier(")
    # for param, value in study.best_params.items():
    #     print(f"    {param}={value},")
    #     print("    random_state=42,")
    #     print("    n_jobs=-1")
    #     print(")")

    ################################################################################
    # END: fix logs for params
    ################################################################################

    # mlflow.log_params('class_weight_1', study.best_params['class_weight_1'])
    # mlflow.log_params('class_weight_2', study.best_params['class_weight_2'])
    mlflow.log_metric("best_F1-macro_score_fine_tuning", study.best_value)
    
    # Train best model
    # merged_params = merge_params(initial_forest_model_parameters, study.best_params)
    # # Remove class_weight keys
    merged_params = initial_forest_model_parameters | study.best_params
    del merged_params['class_weight_1']
    del merged_params['class_weight_2']

    print("merged_params:", merged_params)

    mlflow.log_params(merged_params)
    
    # Train the Final Model with Best Parameters
    rf_model = RandomForestClassifier(**merged_params)
    rf_model.fit(attributes_train, target_train)

    # Removing irrelevant features (feature selection)
    attributes_train_reg = remove_low_important_features(rf_model, attributes_train, attributes_type = 'train')
    attributes_test_reg = remove_low_important_features(rf_model, attributes_test, attributes_type = 'test')

    print(f"High important features after feature selection (regularization): ", attributes_train_reg.columns)

    # Train the Final Model with Best Parameters and without the low important featires
    rf_model_reg = RandomForestClassifier(**merged_params)
    rf_model_reg.fit(attributes_train_reg, target_train)

    # Validate
    target_train_pred = rf_model_reg.predict(attributes_train_reg)
    classification_train_report = classification_report(target_train, target_train_pred)

    target_test_pred = rf_model_reg.predict(attributes_test_reg)
    classification_test_report = classification_report(target_test, target_test_pred)
   
    print(f"classification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
    print(f"classification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)

    # F1-macro score after the regularization
    f1_macro_score = f1_score(target_test, target_test_pred, average='macro')
    print(f"F1-Score (Macro): {f1_macro_score:.4f}")
    
    mlflow.log_metric("F1-macro-score-important-features", f1_macro_score)

    # Log metrics
    mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", rf_model_reg.score(attributes_train_reg, target_train))
    mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", rf_model_reg.score(attributes_test_reg, target_test))

    with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_train_report)
    with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_test_report)
        
    pickle.dump(rf_model_reg, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))

    # Log artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
    
    classes = [0, 1, 2]
    #evaluate_model(rf_model_reg, attributes_test, target_test, classes)


with mlflow.start_run(run_name = mlflow_run_name):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))

    # Create a Optuna study and optimize
    study = optuna.create_study(direction='maximize')
    study.optimize(
        partial(objective, model_type = model_type, oversampling_type = oversampling_type, n_splits = n_splits),
        n_trials = n_trials
    )

    # Visualize optimization history
    optimization_history_plot = vis.plot_optimization_history(study)
    # Visualize parameter importance
    param_importance_plot = vis.plot_param_importances(study)
    # Visualize hyperparameter values
    hyperparameter_values_plot = vis.plot_parallel_coordinate(study)

    # Save visualizations to files
    optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Log visualization MLflow artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Output the best hyperparameters
    print("Best Parameters:", study.best_params)
    print("Best F1-macro-score-fine-tuning:", study.best_value)
    print("Best Trial:", study.best_trial)

    # print("\nRecommended hyperparameters for subsequent use:")
    # print("rf_model = RandomForestClassifier(")
    # for param, value in study.best_params.items():
    #     print(f"    {param}={value},")
    #     print("    random_state=42,")
    #     print("    n_jobs=-1")
    #     print(")")

    # mlflow.log_params('class_weight_1', study.best_params['class_weight_1'])
    # mlflow.log_params('class_weight_2', study.best_params['class_weight_2'])
    mlflow.log_metric("best_F1-macro_score_fine_tuning", study.best_value)
    
    # Train best model
    # merged_params = merge_params(initial_forest_model_parameters, study.best_params)
    # # Remove class_weight keys
    merged_params = initial_forest_model_parameters | study.best_params
    del merged_params['class_weight_1']
    del merged_params['class_weight_2']

    print("merged_params:", merged_params)

    mlflow.log_params(merged_params)
    
    # Train the Final Model with Best Parameters
    rf_model = RandomForestClassifier(**merged_params)
    rf_model.fit(attributes_train, target_train)

    # Removing irrelevant features (feature selection)
    attributes_train_reg = remove_low_important_features(rf_model, attributes_train, attributes_type = 'train')
    attributes_test_reg = remove_low_important_features(rf_model, attributes_test, attributes_type = 'test')

    print(f"High important features after feature selection (regularization): ", attributes_train_reg.columns)

    # Train the Final Model with Best Parameters and without the low important featires
    rf_model_reg = RandomForestClassifier(**merged_params)
    rf_model_reg.fit(attributes_train_reg, target_train)

    # Validate
    target_train_pred = rf_model_reg.predict(attributes_train_reg)
    classification_train_report = classification_report(target_train, target_train_pred)

    target_test_pred = rf_model_reg.predict(attributes_test_reg)
    classification_test_report = classification_report(target_test, target_test_pred)
   
    print(f"classification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
    print(f"classification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)

    # F1-macro score after the regularization
    f1_macro_score = f1_score(target_test, target_test_pred, average='macro')
    print(f"F1-Score (Macro): {f1_macro_score:.4f}")
    
    mlflow.log_metric("F1-macro-score-important-features", f1_macro_score)

    # Log metrics
    mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", rf_model_reg.score(attributes_train_reg, target_train))
    mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", rf_model_reg.score(attributes_test_reg, target_test))

    with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_train_report)
    with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_test_report)
        
    pickle.dump(rf_model_reg, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))

    # Log artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
    
    classes = [0, 1, 2]
    #evaluate_model(rf_model_reg, attributes_test, target_test, classes)


with mlflow.start_run(run_name = mlflow_run_name):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))

    # Create a Optuna study and optimize
    study = optuna.create_study(direction='maximize')
    study.optimize(
        partial(objective, model_type = model_type, oversampling_type = oversampling_type, n_splits = n_splits),
        n_trials = n_trials
    )

    # Visualize optimization history
    optimization_history_plot = vis.plot_optimization_history(study)
    # Visualize parameter importance
    param_importance_plot = vis.plot_param_importances(study)
    # Visualize hyperparameter values
    hyperparameter_values_plot = vis.plot_parallel_coordinate(study)

    # Save visualizations to files
    optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Log visualization MLflow artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

    # Output the best hyperparameters
    print("Best Parameters:", study.best_params)
    print("Best F1-macro-score:", study.best_value)
    print("Best Trial:", study.best_trial)

    # mlflow.log_params('class_weight_1', study.best_params['class_weight_1'])
    # mlflow.log_params('class_weight_2', study.best_params['class_weight_2'])
    mlflow.log_metric("best_F1-macro-score", study.best_value)
    
    # Train best model
    # merged_params = merge_params(initial_forest_model_parameters, study.best_params)
    # # Remove class_weight keys
    merged_params = initial_forest_model_parameters | study.best_params
    del merged_params['class_weight_1']
    del merged_params['class_weight_2']

    print("merged_params:", merged_params)

    mlflow.log_params(merged_params)
    
    # Train the Final Model with Best Parameters
    rf_model = RandomForestClassifier(**merged_params)
    rf_model.fit(attributes_train, target_train)

    # Removing irrelevant features (feature selection)
    attributes_train_reg = remove_low_important_features(rf_model, attributes_train, attributes_type = 'train')
    attributes_test_reg = remove_low_important_features(rf_model, attributes_test, attributes_type = 'test')

    print(f"High important features after feature selection (regularization): ", attributes_train_reg.columns)

    # Train the Final Model with Best Parameters and without the low important featires
    rf_model_reg = RandomForestClassifier(**merged_params)
    rf_model_reg.fit(attributes_train_reg, target_train)

    # Validate
    target_train_pred = rf_model_reg.predict(attributes_train_reg)
    classification_train_report = classification_report(target_train, target_train_pred)

    target_test_pred = rf_model_reg.predict(attributes_test_reg)
    classification_test_report = classification_report(target_test, target_test_pred)
   
    print(f"classification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
    print(f"classification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)

    # F1-macro score after the regularization
    f1_macro_score = f1_score(target_test, target_test_pred, average='macro')
    print(f"F1-Score (Macro): {f1_macro_score:.4f}")
    
    mlflow.log_metric("Final-F1-macro-score", f1_macro_score)

    # Log metrics
    mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", rf_model_reg.score(attributes_train_reg, target_train))
    mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", rf_model_reg.score(attributes_test_reg, target_test))

    with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_train_report)
    with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
        f.write(classification_test_report)
        
    pickle.dump(rf_model_reg, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))

    # Log artifacts
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
    mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
    
    classes = [0, 1, 2]
    #evaluate_model(rf_model_reg, attributes_test, target_test, classes)








from sklearn.pipeline import Pipeline

X = attributes
y = target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = False)

# Define pipeline with placeholder for scaler and model
pipeline = Pipeline([
    ('scaler', None),  # Placeholder for scaler
    ('rf', RandomForestClassifier(random_state=42))
])

# Define parameter grid for grid search
param_grid = {
    'scaler': [None, MinMaxScaler(), StandardScaler()],  # Test with and without scaling
    'rf__n_estimators': [100, 300],                # Number of trees
    # 'rf__n_estimators': [100, 300, 500],                # Number of trees
    'rf__max_depth': [None, 10, 20, 30],                # Tree depth
    'rf__min_samples_split': [2, 5, 10],                # Min samples required to split
    'rf__min_samples_leaf': [1, 2, 4],                  # Min samples required at leaf node
    'rf__max_features': ['sqrt', 'log2', None],         # Max features considered for split
    'rf__bootstrap': [True, False]                      # Use bootstrap sampling or not
}

# Perform grid search
# grid_search = GridSearchCV(
#     pipeline,
#     param_grid,
#     cv=2,                # 3-fold cross-validation
#     # cv=3,                # 3-fold cross-validation
#     scoring='accuracy',  # Optimize for accuracy
#     verbose=2,
#     n_jobs=-9          
#     # n_jobs=-1            # Use all available CPU cores
# )

# Fit grid search
grid_search.fit(X_train, y_train)

# Output best parameters and score
print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

# Evaluate on test set
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print("Test set accuracy:", test_score)









def print_classification_report_multiclass(model, attributes_test_data, target_test_data): 
    # Predictions
    target_pred_data = model.predict(attributes_test_data)

    # Calculate F1-score (weighted)
    f1 = f1_score(target_test_data, target_pred_data, average='weighted')

    print(f"F1 score (weighted) : [{f1}]")
    
    # Classification Report
    print("Classification report: ")
    print(classification_report(target_test_data, target_pred_data))


def plot_classification_error(model, eval_metric='mlogloss'):
    # Retrieve evaluation results
    eval_results = model.evals_result()
    
    # Plot classification error
    plt.figure(figsize=(10, 6))
    plt.plot(eval_results['validation_0'][eval_metric], label='Train Error')
    plt.plot(eval_results['validation_1'][eval_metric], label='Validation Error')
    plt.xlabel('Boosting Rounds')
    plt.ylabel('Classification Error')
    plt.title(f"Classification Error ({eval_metric}) During Training ")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_roc_curve_multiclass(model,target_test_data, classes):
    print(f"Calculate the ROC-AUC score for each class individually to understand how well the model performs for the minority classes")
    
    # Binarize labels for one-vs-rest (multiclass ROC-AUC)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities

    # ROC-AUC for multiclass
    roc_auc = roc_auc_score(target_test, target_pred_prob, multi_class='ovr') # 'ovr' (one-vs-rest), 'ovo'-(one-vs-one)
    print(f"ROC-AUC Score: {roc_auc}")
    
    # Compute ROC-AUC for each class
    roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
    print(f"ROC-AUC per class {classes}:", roc_auc_per_class)

    # Compute ROC curve and ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot ROC curves for each class
    plt.figure()
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    
    # Configure the plot
    plt.title('ROC-AUC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()


def plot_precision_recal_curve_multiclass(model,target_test_data, classes):
    # Binarize labels for one-vs-rest (multiclass precision-recall)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities
    
    # Plot precision-recall curves for each class
    for i in range(3):  # Assuming 3 classes (0, 1, 2)
        precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        plt.plot(recall, precision, label=f'Class {i}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()


def plot_feature_importance(model):
    # Feature importances
    importances = model.feature_importances_
    feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title('Feature Importance')
    plt.show()


def evaluate_model(model, attributes_test_data, target_test_data, classes):
    print_classification_report_multiclass(model, attributes_test_data, target_test_data)
    # plot_classification_error(model, eval_metric='mlogloss')
    # plot_classification_error(model, eval_metric='merror')
    plot_roc_curve_multiclass(model, target_test_data, classes)
    plot_precision_recal_curve_multiclass(model, target_test_data, classes)
    plot_feature_importance_xgb(model)


classes = [0, 1, 2]
#evaluate_model(rf_model, attributes_test, target_test, classes)





def run_time_series_cross_validation(attributes_data, oversampling_type = None, n_splits = 5):
    """
    Runs a cross validation for time series data. The 'attributes_data' is splitted with a 'TimeSeriesSplit' in 'n_splits'-folds
    
    Parameters:
    ----------
    attributes_data: attributes data for the cross validation
    n_splits : number of splis the data for the cross validation
    oversampling_type : applies oversampling to generate synthetic examples for minority classes. Possible values: 'adasyn' or 'smote'
    """ 
    
    print(f"Start time series cross validation with oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")
    
    # Use TimeSeriesSplit for cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)

    # Initialize metrics
    f1_macro_scores = []
    f1_weighted_scores = []
    balanced_acc_scores = []
    kappa_scores = []
    log_loss_scores = []
    roc_auc_scores = []

    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes_data)):
        print(f"Fold {fold + 1}")

        # Get data for the current fold
        attributes_train_fold, attributes_val_fold = attributes.iloc[train_idx], attributes.iloc[val_idx]
        target_train_fold, target_val_fold = target.iloc[train_idx], target.iloc[val_idx]

        # Use oversampling
        attributes_train_fold, target_train_fold = create_oversampling(
            attributes_train_fold,
            target_train_fold, 
            oversampling_type = oversampling_type
        )
        
        # Train model with the Initial hyper params
        rf_model_split = RandomForestClassifier(**initial_forest_model_parameters)
        rf_model_split.fit(attributes_train_fold, target_train_fold)
    
        # Predictions and probabilities
        target_val_pred = rf_model_split.predict(attributes_val_fold)
        target_val_proba = rf_model_split.predict_proba(attributes_val_fold)

        # Calculate metrics for this fold
        f1_macro_scores.append(f1_score(target_val_fold, target_val_pred, average='macro'))
        f1_weighted_scores.append(f1_score(target_val_fold, target_val_pred, average='weighted'))
        balanced_acc_scores.append(balanced_accuracy_score(target_val_fold, target_val_pred))
        kappa_scores.append(cohen_kappa_score(target_val_fold, target_val_pred))
        log_loss_scores.append(log_loss(target_val_fold, target_val_proba))
        roc_auc_scores.append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))
        
        print(f"Fold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))

    # Aggregate metrics across folds
    print("\nCross-Validated Metrics:")
    print(f"F1-Score (Macro): {np.mean(f1_macro_scores):.4f} ± {np.std(f1_macro_scores):.4f}")
    print(f"F1-Score (Weighted): {np.mean(f1_weighted_scores):.4f} ± {np.std(f1_weighted_scores):.4f}")
    print(f"Balanced Accuracy: {np.mean(balanced_acc_scores):.4f} ± {np.std(balanced_acc_scores):.4f}")
    print(f"Cohen's Kappa: {np.mean(kappa_scores):.4f} ± {np.std(kappa_scores):.4f}")
    print(f"Log Loss: {np.mean(log_loss_scores):.4f} ± {np.std(log_loss_scores):.4f}")
    print(f"ROC-AUC: {np.mean(roc_auc_scores):.4f} ± {np.std(roc_auc_scores):.4f}")


run_time_series_cross_validation(attributes, oversampling_type, n_splits = 5)


run_time_series_cross_validation(attributes, oversampling_type, n_splits = 5)
