





# Install XGBoost if not already installed
# !pip install xgboost

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Generate synthetic time series data
np.random.seed(42)
time_series_length = 100
data = pd.DataFrame({
    'value': np.sin(np.linspace(0, 3 * np.pi, time_series_length)) + np.random.normal(scale=0.1, size=time_series_length)
})

# Create lag features (past values as features) for supervised learning
def create_lag_features(df, lag=3):
    for i in range(1, lag + 1):
        df[f'lag_{i}'] = df['value'].shift(i)
    df.dropna(inplace=True)
    return df

# Define the number of past time steps to use as features
lag = 3
data = create_lag_features(data, lag)

# Split the data into train and test sets
X = data.drop('value', axis=1)  # Features are the lagged values
y = data['value']               # Target is the current value

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Initialize the XGBoost regressor
model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=3, learning_rate=0.1)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Plot the actual vs predicted values
plt.figure(figsize=(10, 6))
plt.plot(range(len(y_test)), y_test, label='Actual', color='blue')
plt.plot(range(len(y_pred)), y_pred, label='Predicted', color='red')
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.title("Time Series Forecasting with XGBoost")
plt.legend()
plt.show()











# Install XGBoost if not already installed
# !pip install xgboost

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Generate synthetic time series data
np.random.seed(42)
time_series_length = 100
data = pd.DataFrame({
    'value': np.sin(np.linspace(0, 3 * np.pi, time_series_length)) + np.random.normal(scale=0.1, size=time_series_length)
})

# Create lag features (past values as features) for supervised learning
def create_lag_features(df, lag=3):
    for i in range(1, lag + 1):
        df[f'lag_{i}'] = df['value'].shift(i)
    df.dropna(inplace=True)
    return df

# Define the number of past time steps to use as features
lag = 3
data = create_lag_features(data, lag)

# Split the data into train and test sets
X = data.drop('value', axis=1)  # Features are the lagged values
y = data['value']               # Target is the current value

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Initialize the XGBoost regressor
model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=3, learning_rate=0.1)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Plot the actual vs predicted values
plt.figure(figsize=(10, 6))
plt.plot(range(len(y_test)), y_test, label='Actual', color='blue')
plt.plot(range(len(y_pred)), y_pred, label='Predicted', color='red')
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.title("Time Series Forecasting with XGBoost")
plt.legend()
plt.show()


data








# Install XGBoost if not already installed
# !pip install xgboost

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Generate synthetic time series data
np.random.seed(42)
time_series_length = 100
data = pd.DataFrame({
    'value': np.sin(np.linspace(0, 3 * np.pi, time_series_length)) + np.random.normal(scale=0.1, size=time_series_length)
})

# Create lag features for supervised learning
def create_lag_features(df, lag=3):
    for i in range(1, lag + 1):
        df[f'lag_{i}'] = df['value'].shift(i)
    df.dropna(inplace=True)
    return df

# Define the number of past time steps to use as features
lag = 3
data = create_lag_features(data, lag)

# Separate the training set
train_data = data[:-10]  # Using the last 10 points for recursive forecasting
X_train = train_data.drop('value', axis=1)
y_train = train_data['value']

# Initialize and train the XGBoost regressor
model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=3, learning_rate=0.1)
model.fit(X_train, y_train)

# Recursive Forecasting
n_forecasts = 10  # Number of future steps to forecast
last_values = data.drop('value', axis=1).iloc[-1].values  # Last known values to start forecasting
predictions = []

for i in range(n_forecasts):
    # Predict the next value
    pred = model.predict(last_values.reshape(1, -1))[0]
    predictions.append(pred)
    
    # Update the last_values array by removing the oldest lag and adding the new prediction
    last_values = np.roll(last_values, -1)
    last_values[-1] = pred

# Evaluate the model's forecasting accuracy on the test set
actual_values = data['value'].iloc[-n_forecasts:].values
mse = mean_squared_error(actual_values, predictions)
print("Mean Squared Error:", mse)

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(range(len(data['value'])), data['value'], label='Actual Data', color='blue')
plt.plot(range(len(data['value']) - n_forecasts, len(data['value'])), predictions, label='Predictions', color='red')
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.title("Recursive Forecasting with XGBoost")
plt.legend()
plt.show()



