import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from functools import partial

import pickle
import mlflow
import time
import logging

import optuna
import optuna.visualization as vis

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from sklearn.utils.class_weight import compute_class_weight

# Scalers
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE, ADASYN

# Metrics
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, auc, roc_curve, precision_recall_curve, f1_score, confusion_matrix, balanced_accuracy_score, cohen_kappa_score, log_loss 

# Model
import lightgbm as lgb
from lightgbm import LGBMClassifier














attributes = pd.read_csv("data/btc_price_data_1_year_attributes.csv")
attributes.set_index('timestamp', inplace=True, drop = True)
attributes


target_data = pd.read_csv("data/btc_price_data_1_year_target.csv")
target_data.set_index('timestamp', inplace=True, drop = True)
target = target_data['target']
target





attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape























def create_oversampling_smote(attributes_train, target_train):
    """
    Apply SMOTE(Synthetic Minority Oversampling Technique) to generate synthetic examples for minority classes
    """
    smote = SMOTE(random_state=42)
    attributes_train_smote, target_train_smote = smote.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (SMOTE):", np.bincount(target_train))
    print("Balanced Class Distribution (SMOTE):", np.bincount(target_train_smote))

    return attributes_train_smote, target_train_smote





def create_oversampling_adasyn(attributes_train, target_train):
    """
    Apply ADASYN(Adaptive Synthetic Sampling) to generate synthetic examples for minority classes
    """
    adasyn = ADASYN(random_state=42)
    attributes_train_adasyn, target_train_adasyn = adasyn.fit_resample(attributes_train, target_train)
    
    print("Original Class Distribution (ADASYN):", np.bincount(target_train))
    print("Balanced Class Distribution (ADASYN):", np.bincount(target_train_adasyn))

    return attributes_train_adasyn, target_train_adasyn





def create_oversampling(attributes_train, target_train, oversampling_type = None):
    """
    Apply 'oversampling_type' to generate synthetic examples for minority classes
    """
    if oversampling_type == 'smote':
        return create_oversampling_smote(attributes_train, target_train)
    elif oversampling_type == 'adasyn':
        return create_oversampling_adasyn(attributes_train, target_train)
    else:
        print(f"Invalid oversampling type: [{oversampling_type}]")
        return attributes_train, target_train














def plot_feature_importance(model, importances, feature_names, title = 'Feature Importance'):
    # Feature importances
    # importances = model.feature_importances_
    # feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title(title)
    plt.show()


def remove_low_important_features(model, attributes, attributes_type):
    importances = model.feature_importances_
    feature_names = attributes.columns
    
    # Plot the importances
    plot_feature_importance(model, importances, feature_names, title = f'Feature Importance({attributes_type}) before drop')
    
    # Feature importance
    feature_importances = pd.DataFrame({
        'Feature': attributes.columns,
        'Importance': model.feature_importances_
    }).sort_values(by="Importance", ascending=False)
    
    # Drop features with low importance
    low_importance_features = feature_importances[feature_importances['Importance'] < 0.01]['Feature']
    attributes_reg = attributes.drop(columns=low_importance_features)

    print('attributes.shape before drop: ' , attributes.shape)
    print('attributes.shape after the drop: ', attributes_reg.shape)

    # Plot the importances after the drop
    # plot_feature_importance(model, high_importance_features.Importance, high_importance_features.Feature, title = 'Feature Importance after the drop')

    return attributes_reg


# attributes_reg = remove_low_important_features(lgb_model, attributes, 'train')





def plot_classification_report_multiclass(model, attributes_test_data, target_test_data): 
    # Predictions
    target_pred_data = model.predict(attributes_test_data, verbose=-1)

    # Calculate F1-score (weighted)
    f1 = f1_score(target_test_data, target_pred_data, average='macro')

    print(f"\nF1 score (macro) : [{f1}]")
    
    # Classification Report
    print("\nClassification report: ")
    print(classification_report(target_test_data, target_pred_data))


# Plot LIghtLGB Feature Importance
def plot_lgb_importance(lgb_model):
    lgb.plot_importance(lgb_model, max_num_features=20, importance_type = 'split', title = 'Feature Importance (split)')
    lgb.plot_importance(lgb_model, max_num_features=20, importance_type = 'gain', title = 'Feature Importance (gain)')
    plt.show()


def plot_confusion_matrix(model, attributes_test_data, target_test_data):
    # Predictions
    target_pred_data = model.predict(attributes_test_data, verbose=-1)
    
    # Confusion Matrix Computation
    cm = confusion_matrix(target_test_data, target_pred_data)
    #print("Confusion Matrix:\n", cm)
    
    # Confusion Matrix Visualization
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], 
                yticklabels=['Class 0', 'Class 1', 'Class 2'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()


def plot_classification_error(model, eval_metric='multi_logloss'):
    # Retrieve evaluation results
    eval_results = model.evals_result_
    
    # Plot classification error
    plt.figure(figsize=(10, 6))
    plt.plot(eval_results['training'][eval_metric], label='Train Error')
    plt.plot(eval_results['valid_1'][eval_metric], label='Validation Error')
    plt.xlabel('Boosting Rounds')
    plt.ylabel('Classification Error')
    plt.title(f"Classification Error ({eval_metric}) During Training ")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_roc_curve_multiclass(model,target_test_data, classes):
    print(f"\nCalculate the ROC-AUC score for each class individually to see how well the model performs for the minority classes")
    
    # Binarize labels for one-vs-rest (multiclass ROC-AUC)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities

    # ROC-AUC for multiclass
    roc_auc = roc_auc_score(target_test, target_pred_prob, multi_class='ovr') # 'ovr' (one-vs-rest), 'ovo'-(one-vs-one)
    print(f"\nROC-AUC Score: {roc_auc}")
    
    # Compute ROC-AUC for each class
    roc_auc_per_class = roc_auc_score(target_test_binarized, target_pred_prob, average=None)
    print(f"ROC-AUC per class {classes}:", roc_auc_per_class)

    # Compute ROC curve and ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot ROC curves for each class
    plt.figure()
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc[i]:.2f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    
    # Configure the plot
    plt.title('ROC-AUC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()


def plot_precision_recal_curve_multiclass(model,target_test_data, classes):
    # Binarize labels for one-vs-rest (multiclass precision-recall)
    target_test_binarized = label_binarize(target_test_data, classes = classes)
    target_pred_prob = model.predict_proba(attributes_test) # predicted probabilities
    
    # Plot precision-recall curves for each class
    for i in range(3):  # Assuming 3 classes (0, 1, 2)
        precision, recall, _ = precision_recall_curve(target_test_binarized[:, i], target_pred_prob[:, i])
        plt.plot(recall, precision, label=f'Class {i}')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()


def plot_feature_importance(model):
    # Feature importances
    importances = model.feature_importances_
    feature_names = attributes_train.columns
    sorted_indices = importances.argsort()
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices])
    plt.title('Feature Importance')
    plt.show()


def evaluate_model(model, attributes_test_data, target_test_data):
    classes = [0, 1, 2]
    
    plot_classification_report_multiclass(model, attributes_test_data, target_test_data)
    plot_lgb_importance(model)
    plot_confusion_matrix(model, attributes_test_data, target_test_data)
    plot_classification_error(model, eval_metric='multi_logloss')
    # plot_classification_error(model, eval_metric='merror')
    plot_roc_curve_multiclass(model, target_test_data, classes)
    plot_precision_recal_curve_multiclass(model, target_test_data, classes)


# evaluate_model(lgb_model, attributes_test, target_test)





initial_lgbm_model_parameters = {
    'random_state': 42,
    'n_jobs': 10,
    'boosting_type': 'dart',
    'learning_rate': 0.05,
    'num_leaves': 244,
    'min_data_in_leaf': 95,
    'max_depth': 10,
    'feature_fraction': 0.63,
    'bagging_fraction': 0.78,
    'bagging_freq': 5,
    'lambda_l1': 1.7663008643545686e-08,
    'lambda_l2': 0.0026352243736742936,
    'n_estimators': 300
}














def train_model(model_params, attributes_train_data, target_train_data, attributes_test_data, target_test_data, oversampling_type = None, log_evaluation_period = 10):
    print("\nmodel_params:", model_params)
    
    # Apply oversampling to handle class imbalance in training data
    attributes_train_data_oversampling, target_train_data_oversampling = create_oversampling(
            attributes_train_data,
            target_train_data, 
            oversampling_type
    )
    
    # Use both the training data and validation data in the eval_set to monitor both training and validation performance during training
    eval_set = [
        (attributes_train_data_oversampling, target_train_data_oversampling), # Resampled training data
        (attributes_test_data, target_test_data)    # Original validation data
    ]
    
    # Train the final model on all available data (training + test)
    lgb_model = LGBMClassifier(**model_params, verbose = -1)
    lgb_model.fit(
        attributes_train_data_oversampling, # oversampling-applied training data
        target_train_data_oversampling,     # oversampling-applied training data
        eval_set= eval_set,
        callbacks=[
            lgb.early_stopping(stopping_rounds = 50),
            lgb.log_evaluation(period = log_evaluation_period)  # Optional: control logging
        ])

    # Evaluate the model
    # evaluate_model(lgb_model, attributes_test_data, target_test_data)
    
    return lgb_model


def fit_model_with_timeseries_split(model_params, attributes_train_data, target_train_data, attributes_test_data, target_test_data, oversampling_type = None, log_evaluation_period = 10):
    """
    Cross-Validate with TimeSeriesSplit - using TimeSeriesSplit on the training data to ensure 
    the model performs consistently across different time windows.
    """
     # Metrics collection
    scores = {
        #  The F1 score is the harmonic mean of precision and recall. The 'macro' version calculates the F1 score 
        #  for each class independently and then averages them, giving equal weight to all classes, regardless of their size
        'f1_macro_scores' : [],
        # Balanced accuracy is the average of recall obtained on each class. It accounts for class imbalance 
        # by giving equal weight to each class
        'balanced_acc_scores' : [],
        'f1_weighted_scores' : [],
        'kappa_scores' : [],
        'log_loss_scores' : [],
        'roc_auc_scores' : [],
    }

    # Use TimeSeriesSplit for time series cross-validation
    tscv = TimeSeriesSplit(n_splits = n_splits)
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(attributes_train_data)):
        attributes_train_fold, attributes_val_fold = attributes_train_data.iloc[train_idx], attributes_train_data.iloc[val_idx]
        target_train_fold, target_val_fold = target_train_data.iloc[train_idx], target_train_data.iloc[val_idx]

        # Train the LightGB model with the current fold data
        lgb_model_split = train_model(
            model_params = model_params,
            attributes_train_data = attributes_train_fold,
            target_train_data = target_train_fold,
            attributes_test_data = attributes_test_data,
            target_test_data = target_test_data, 
            oversampling_type = oversampling_type,
            log_evaluation_period = log_evaluation_period)

        # Predictions and probabilities
        target_val_pred = lgb_model_split.predict(attributes_val_fold)
        target_val_proba = lgb_model_split.predict_proba(attributes_val_fold)
        
        # Calculate metrics for this fold
        scores['f1_macro_scores'].append(f1_score(target_val_fold, target_val_pred, average='macro'))
        scores['f1_weighted_scores'].append(f1_score(target_val_fold, target_val_pred, average='weighted'))
        scores['balanced_acc_scores'].append(balanced_accuracy_score(target_val_fold, target_val_pred))
        scores['kappa_scores'].append(cohen_kappa_score(target_val_fold, target_val_pred))
        scores['log_loss_scores'].append(log_loss(target_val_fold, target_val_proba))
        scores['roc_auc_scores'].append(roc_auc_score(target_val_fold, target_val_proba, multi_class='ovr'))

        print(f"\nFold {fold+1} Classification Report")
        print(classification_report(target_val_fold, target_val_pred))

    # Aggregate metrics across folds
    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}

    print("\nCross-Validated Metrics:")
    print(f"F1-Score (Macro): {avg_scores['f1_macro_scores']:.4f} ± {np.std(scores['f1_macro_scores']):.4f}")
    print(f"F1-Score (Weighted): {avg_scores['f1_weighted_scores']:.4f} ± {np.std(scores['f1_weighted_scores']):.4f}")
    print(f"Balanced Accuracy: {avg_scores['balanced_acc_scores']:.4f} ± {np.std(scores['balanced_acc_scores']):.4f}")
    print(f"Cohen's Kappa: {avg_scores['kappa_scores']:.4f} ± {np.std(scores['kappa_scores']):.4f}")
    print(f"Log Loss: {avg_scores['log_loss_scores']:.4f} ± {np.std(scores['log_loss_scores']):.4f}")
    print(f"ROC-AUC: {avg_scores['roc_auc_scores']:.4f} ± {np.std(scores['roc_auc_scores']):.4f}")

    return avg_scores


def objective(trial, model_type, oversampling_type = None, n_splits = 5, log_evaluation_period = 10):
    print(f"\nStart trial:[{trial.number}] for model:[{model_type}] with time series cross validation, oversampling:[{oversampling_type}] and n_splits:[{n_splits}]")

    # Define the hyperparameters to tune
    params = {
        'objective': 'multiclass',
        'metric': 'multi_logloss',
        'num_class': 3,
        'boosting_type': trial.suggest_categorical('boosting_type', ['dart']), # 'gbdt', 'dart'
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 31, 256),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 100),
        'max_depth': trial.suggest_int('max_depth', -1, 15),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0, log=True),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0, log=True),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        'class_weight' : 'balanced',
        "n_estimators": trial.suggest_int("n_estimators", 50, 500),  # Variable n_estimators
        # "n_estimators": trial.suggest_int("n_estimators", 50, 1000),  # Variable n_estimators
        'n_jobs': 10,
        'random_state': 42
    }
    
    # Train the model with the 'params'
    avg_scores = fit_model_with_timeseries_split(params, attributes_train, target_train, attributes_test, target_test, oversampling_type, log_evaluation_period)

    # Log all scores to Optuna
    trial.set_user_attr("scores", avg_scores)
    
    # Return the mean F1-macro across all folds
    return avg_scores['f1_macro_scores']












# Set model type
model_type = 'LightGBM'

# Set oversampling type
oversampling_type = 'adasyn'

n_trials = 50 # default : 50
n_splits = 5 # default : 5
posfix = '_DART_balanced_weight_test_2'
mlflow_run_name = f"BTC_{model_type}_Optuna_{oversampling_type}_Trials_{n_trials}{posfix}"

print("Oversampling Type:", oversampling_type)
print("n_trials:", n_trials)
print("n_splits:", n_splits)
print("mlflow_run_name:", mlflow_run_name)


# Create a MLFlow experiment
experiment_name = f"BTC_{model_type}_Optuna"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None:
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)





def start_mlflow_run(run_name, use_optuna = True, n_splits = 5, initial_lgbm_model_parameters = initial_lgbm_model_parameters, log_evaluation_period = 10):
    with mlflow.start_run(run_name = run_name):
        # Suppress only warning logs from LightGBM
        #logging.getLogger('LightGBM').setLevel(logging.ERROR)
    
        mlflow.log_param("train size", len(attributes_train))
        mlflow.log_param("test size", len(attributes_test))

        merged_params = initial_lgbm_model_parameters
    
        if use_optuna:
            # Create a Optuna study and optimize
            study = optuna.create_study(direction = 'maximize')
            study.optimize(
                partial(objective, model_type = model_type, oversampling_type = oversampling_type, n_splits = n_splits),
                n_trials = n_trials
            )
    
            mlflow.log_metric("best_F1-macro_score_fine_tuning", study.best_value)
            
            # Visualize optimization history
            optimization_history_plot = vis.plot_optimization_history(study)
            # Visualize parameter importance
            param_importance_plot = vis.plot_param_importances(study)
            # Visualize hyperparameter values
            hyperparameter_values_plot = vis.plot_parallel_coordinate(study)
        
            # Save visualizations to files
            optimization_history_plot.write_html(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
            param_importance_plot.write_html(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
            hyperparameter_values_plot.write_html(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")
        
            # Log visualization MLflow artifacts
            mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/optimization_history_{model_type}_{oversampling_type}.html")
            mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/param_importances_{model_type}_{oversampling_type}.html")
            mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/hyperparameter_values_{model_type}_{oversampling_type}.html")

            ################################################################################
            # log best params
            ################################################################################
            # Get the best trial
            best_trial = study.best_trial
        
            # Output the best hyperparameters
            print("\n\nBest Parameters:", study.best_params)
            print("\nBest F1-macro-score-fine-tuning:", study.best_value)
            print("\nBest Trial:", best_trial)
        
            # Access the parameters of the best trial
            print("\n\nBest Trial Parameters:")
            print(f"Trial {best_trial.number}:")
            print(f"Parameters: {best_trial.params}")
            
           # Access the scores stored in user attributes
            print("\nBest Trial Scores:")
            for metric, value in best_trial.user_attrs['scores'].items():
                print(f"{metric}: {value:.4f}")
                mlflow.log_metric(metric, value)
        
            ################################################################################
            # END: log best params
            ################################################################################
            
            # Get the best hyper params from the best model
            merged_params = initial_lgbm_model_parameters | study.best_params
            print("\nmerged_params:", merged_params)
        
        mlflow.log_params(merged_params)
    
        # # Removing irrelevant features (feature selection)
        # attributes_train_reg = remove_low_important_features(rf_model, attributes_train, attributes_type = 'train')
        # attributes_test_reg = remove_low_important_features(rf_model, attributes_test, attributes_type = 'test')
    
        # Train the model with the best params and TimeSeriesSplit
        avg_scores = fit_model_with_timeseries_split(merged_params, attributes_train, target_train, attributes_test, target_test, oversampling_type, log_evaluation_period)
    
        print(f"\nF1-Score(Macro) with Best Params and TimeSplit: {avg_scores['f1_macro_scores']:.4f}")
        # Access the scores stored in user attributes
        print("\nScores with Best Params and TimeSplit:")
        for metric, value in avg_scores.items():
            print(f"{metric}: {value:.4f}")
            mlflow.log_metric(metric, value)

        # Final training the model with the best params without timesplit
        lgb_model = train_model(merged_params, attributes_train, target_train, attributes_test, target_test, oversampling_type, log_evaluation_period)
        
        # Validate
        target_train_pred = lgb_model.predict(attributes_train)
        target_test_pred = lgb_model.predict(attributes_test)
        
        # Validate
        classification_train_report = classification_report(target_train, target_train_pred)
        classification_test_report = classification_report(target_test, target_test_pred)
       
        print(f"\nclassification_train_report_{model_type}_{oversampling_type}: ", classification_train_report)
        print(f"\nclassification_test_report_{model_type}_{oversampling_type}: ", classification_test_report)
    
        # F1-macro score after the final model tarining on all available data (training + test)
        f1_macro_score = f1_score(target_test, target_test_pred, average='macro')
        print(f"F1-Score (Macro) final training: {f1_macro_score:.4f}")
        
        mlflow.log_metric("F1-macro-score-final", f1_macro_score)
    
        # Log metrics
        mlflow.log_metric(f"train_accuracy_{model_type}_{oversampling_type}", lgb_model.score(attributes_train, target_train))
        mlflow.log_metric(f"test_accuracy_{model_type}_{oversampling_type}", lgb_model.score(attributes_test, target_test))
    
        with open(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt",  "w") as f:
            f.write(classification_train_report)
        with open(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt",  "w") as f:
            f.write(classification_test_report)
            
        pickle.dump(lgb_model, open(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl", "wb"))
    
        # Log artifacts
        mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/model_{model_type}_{oversampling_type}.pkl")
        mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/train_report_{model_type}_{oversampling_type}.txt")
        mlflow.log_artifact(f"data/{model_type}/{oversampling_type}/test_report_{model_type}_{oversampling_type}.txt")
        
        evaluate_model(lgb_model, attributes_test, target_test)


best_model_params_adasyn = {
    'boosting_type': 'dart', 
    'learning_rate': 0.049326936253012287, 
    'num_leaves': 88,
    'min_data_in_leaf': 54, 
    'max_depth': 7,
    'feature_fraction': 0.5530536099334057,
    'bagging_fraction': 0.5697950611829533,
    'bagging_freq': 6,
    'lambda_l1': 0.001592863063329681, 
    'lambda_l2': 3.4073085019992875, 
    'n_estimators': 385,
    'random_state': 42,
    'n_jobs': 10
}

# Best is trial 20 with F1 value: 0.4692108492254034.

# Set oversampling_type to 'adasyn'
oversampling_type = 'adasyn'

n_splits = 2,

# Set mlflow_run_name
posfix = '_train_best_model'
mlflow_run_name = f"BTC_{model_type}_{oversampling_type}{posfix}"
print(mlflow_run_name)


# Train with best params and 'adasyn'
start_mlflow_run(mlflow_run_name, use_optuna = False, n_splits = n_splits, initial_lgbm_model_parameters = best_model_params_adasyn, log_evaluation_period = 50)


best_model_params_smote = {
    'boosting_type': 'dart',
    'learning_rate': 0.13905403484620868,
    'num_leaves': 52,
    'min_data_in_leaf': 73,
    'max_depth': 5,
    'feature_fraction': 0.8541515731770777,
    'bagging_fraction': 0.812277736181733,
    'bagging_freq': 3,
    'lambda_l1': 7.751388497561226,
    'lambda_l2': 0.013244354933658721,
    'n_estimators': 375,
    'random_state': 42,
    'n_jobs': 10
}

# Best is trial 20 with value: 0.4692108492254034.

# Set oversampling_type to 'smote'
oversampling_type = 'smote'

n_splits = 2

# Set mlflow_run_name
posfix = '_train_best_model'
mlflow_run_name = f"BTC_{model_type}_{oversampling_type}{posfix}"
print(mlflow_run_name)


# Train with best params and 'smote'
start_mlflow_run(mlflow_run_name, use_optuna = False, n_splits = n_splits, initial_lgbm_model_parameters = best_model_params_smote, log_evaluation_period = 50)












