import numpy as np
import pandas as pd
# install pandas_ta - https://github.com/twopirllc/pandas-ta
import pandas_ta as ta

import matplotlib.pyplot as plt
import seaborn as sns

import pickle
import mlflow

from statsmodels.tsa.seasonal import STL, seasonal_decompose
from statsmodels.tsa.stattools import pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

from scipy.signal import periodogram

from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance

# Pipelines
from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer

# Metrics
from sklearn.metrics import classification_report, roc_auc_score

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier





























btc_price_data_1_year = pd.read_csv("data/bitcoin_historical_data_1_year.csv")
btc_price_data_1_year


btc_price_data_1_year.describe().T


btc_price_data_1_year.dtypes


btc_price_data_1_year.isnull().sum()











btc_price_data_1_year.timestamp = pd.to_datetime(btc_price_data_1_year.timestamp)





btc_price_data_1_year = btc_price_data_1_year.drop(columns = ["date", "time"])


btc_price_data_1_year.dtypes





btc_price_data_1_year.set_index('timestamp', inplace=True, drop = True)





btc_price_data_1_year['year'] = btc_price_data_1_year.index.year
btc_price_data_1_year['month'] = btc_price_data_1_year.index.month
btc_price_data_1_year['day'] = btc_price_data_1_year.index.day
btc_price_data_1_year['weekday'] = btc_price_data_1_year.index.weekday  # Monday=0, Sunday=6
btc_price_data_1_year['hour'] = btc_price_data_1_year.index.hour





# Extract hour and weekday
btc_price_data_1_year['hour_sin'] = np.sin(2 * np.pi * btc_price_data_1_year.index.hour / 24)  # Hour sin encoding
btc_price_data_1_year['hour_cos'] = np.cos(2 * np.pi * btc_price_data_1_year.index.hour / 24)  # Hour cos encoding
btc_price_data_1_year['weekday_sin'] = np.sin(2 * np.pi * btc_price_data_1_year.index.weekday / 7)  # Weekday sin encoding
btc_price_data_1_year['weekday_cos'] = np.cos(2 * np.pi * btc_price_data_1_year.index.weekday / 7)  # Weekday cos encoding


btc_price_data_1_year.columns


btc_price_data_1_year.dtypes














def calculate_rsi(data, window=14):
    """
    RSI is a momentum oscillator that measures the speed and change of price movements, typically over a 14-period interval.
    Assuming 'data' is a DataFrame with a 'close' price column

    Parameters
    ----------
    data:   a DataFrame with the time series data. A column with the name 'close' must be present in the DataFrame! 
            This column is used to calculate the 'RSI' value.
    window: the time period that is taken into account when calculating the 'RSI'
    """
    delta = data['close'].diff(1)
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    
    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = loss.rolling(window=window, min_periods=1).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    data['RSI'] = rsi
    
    return data


# Calculate RSI
btc_price_data_1_year = calculate_rsi(btc_price_data_1_year)








def calculate_macd(data, short_window=12, long_window=26, signal_window=9):
    """
    Moving Average Convergence Divergence (MACD) is calculated using two exponential moving averages (EMA):
    the 12-day EMA and the 26-day EMA, with a 9-day EMA as the signal line.
    """
    data['EMA12'] = data['close'].ewm(span=short_window, adjust=False).mean()
    data['EMA26'] = data['close'].ewm(span=long_window, adjust=False).mean()
    
    # MACD Line
    data['MACD'] = data['EMA12'] - data['EMA26']
    
    # Signal Line
    data['Signal_Line'] = data['MACD'].ewm(span=signal_window, adjust=False).mean()
    
    return data


# Calculate MACD (Moving Average Convergence Divergence)
btc_price_data_1_year = calculate_macd(btc_price_data_1_year)








def calculate_moving_averages(data, sma_window=20, ema_window=20):
    """
    Simple Moving Average (SMA) is the average price over a specified number of periods, 
    while Exponential Moving Average (EMA) gives more weight to recent prices.
    """
    # Simple Moving Average
    data['SMA'] = data['close'].rolling(window=sma_window).mean()
    
    # Exponential Moving Average
    data['EMA'] = data['close'].ewm(span=ema_window, adjust=False).mean()
    
    return data


# Calculate moving averages
btc_price_data_1_year = calculate_moving_averages(btc_price_data_1_year)














STL(btc_price_data_1_year.close, period = 24).config


seasonal_decompose(btc_price_data_1_year.close, period = 24).plot()


seasonal_decompose(btc_price_data_1_year.close, period = 24).trend.head(20)


# 24h * 30 Days
seasonal_decompose(btc_price_data_1_year.close, period = 24 * 30).plot()


seasonal_decompose(btc_price_data_1_year.close, period = 24 * 30).trend.head(20)








frequencies, power = periodogram(btc_price_data_1_year.close, fs = len(btc_price_data_1_year))
# frequencies, power = periodogram(x, fs=1.0, window='boxcar', nfft=None, detrend='constant', return_onesided=True, scaling='density', axis=-1)


# plt.plot(frequencies, power)
# plt.plot(1/frequencies[1:10000], power[1:10000])
plt.plot(frequencies[:100], power[:100])
# plt.semilogy(frequencies, power)

plt.xlabel("Frequency [Hz]")
plt.ylabel("PSD [Power/Hz]")
plt.title("Power Spectral Density(PSD) of the 'close' column")
plt.show()











lags = 80


plot_acf(btc_price_data_1_year.close, lags = lags)
plt.ylim(-1.2, 1.2)

plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('ACF for \'close\' price')
plt.show()


autocorr = [btc_price_data_1_year['close'].autocorr(lag) for lag in range(1, lags+1)]
sns.heatmap(pd.DataFrame(autocorr, columns=["Autocorrelation"]).T, annot=False, cmap="coolwarm")
plt.xlabel('Lag')
plt.title('Heatmap of Autocorrelation for \'close\' price')
plt.show()





# Calculate PACF
pacf_values = pacf(btc_price_data_1_year['close'], nlags=lags)

# Calculate the confidence interval
conf_int = 1.96 / np.sqrt(len(btc_price_data_1_year['close']))  # The 95% confidence interval

# Get significant lags (values outside the confidence interval)
significant_lags = np.where(np.abs(pacf_values) > conf_int)[0]

#Plot PACF with significant lags marked
plt.stem(pacf_values)

plt.axhline(y=conf_int, color='r', linestyle='--')
plt.axhline(y=-conf_int, color='r', linestyle='--')
plt.ylim(-0.010, 0.010)

plt.grid()

plt.xlabel('Lag')
plt.ylabel('Partial autocorrelation coefficient')
plt.title("PACF with Significant Lags for \'close\' Price'")
plt.show()

# Print the significant lags
print("Significant Lags:", significant_lags)





# Remove the autocorrelation with itself (the '0'-th lag)
significant_lags = significant_lags[1:] if significant_lags[0] == 0 else significant_lags
print("Significant Lags:", significant_lags)


# Create lag features (past values as features) for supervised learning
def create_lag_features(df, lags = []):
    """
    Create lag features (past values as features) for supervised learning
    """
    # Create a copy of the DataFrame
    df_copy = df.copy(deep=True)
    
    for lag in lags:
        df_copy[f'lag_{lag}'] = df_copy['close'].shift(lag)
        
    #df.dropna(inplace=True)
    return df_copy


btc_price_data_1_year = create_lag_features(btc_price_data_1_year, significant_lags)


btc_price_data_1_year.shape


btc_price_data_1_year.columns





def create_target_variable(data, threshold = 0.01):
    """
    Computes and sets the 'target' variable from the input 'data' and 'threshold'.
    Creates a 'target' column with the computed values in the 'data' DataFrame.

    Parameters
    ----------
    data: a DataFrame with the time series data. There must be a column named 'close'! 
          This column will be used by the user to calculate the 'target' variable.
    
    threshold: threshold for the price change to classify as 'buy' or 'sell'. For instance, if you want a 1% increase to be a 'buy' signal,
               the threshold will be 0.01. Adjust this threshold as per your strategy.
    """
    # Create a copy of the DataFrame
    data_copy = data.copy(deep=True)
    
    # Compute the percentage change between the current close price and the close price in the next period.
    # This will help define whether there’s a significant increase or decrease.
    data_copy['future_return'] = ((data_copy['close'].shift(-1) - data_copy['close']) / data_copy['close']) * 100

    # Define the target as 1 (buy) if the future return is above the threshold, and 0 (sell) if it is below or equal to the threshold.
    data_copy['target'] = (data_copy['future_return'] > threshold).astype(float)

    # The last row in your dataset will have a NaN value for 'future_return' due to the shift operation. Drop this row to clean up the dataset.
    #data_copy = data_copy.dropna()

    # Check the balance of 1s and 0s in our target variable to understand how many “buy” and “sell” signals we have.
    print(data_copy['target'].value_counts())

    return data_copy


def create_target_variable_with_techn_indicators(data, price_threshold = 0.01):
    """
    Computes and sets the 'target' variable from the input 'data' and 'threshold'.
    Creates a 'target' column with the computed values in the 'data' DataFrame.

    Parameters
    ----------
    data: a DataFrame with the time series data. There must be a column named 'close'! 
          This column will be used by the user to calculate the 'target' variable.
    
    threshold: threshold for the price change to classify as 'buy' or 'sell'. For instance, if you want a 1% increase to be a 'buy' signal,
               the threshold will be 0.01. Adjust this threshold as per your strategy.
    """
    # Create a copy of the DataFrame
    data_copy = data.copy(deep=True)
    
    # Compute the percentage change between the current close price and the close price in the next period.
    # This will help define whether there’s a significant increase or decrease.
    data_copy['future_return'] = ((data_copy['close'].shift(-1) - data_copy['close']) / data_copy['close']) * 100
    
    # Use the technical indicators to add more conditions to the target:
    # - RSI: A Relative Strength Index (RSI) value below 30 often indicates an oversold condition, which might suggest a buying opportunity.
    # - MACD: A positive MACD value (i.e., MACD > Signal Line) can suggest an uptrend.
    # - SMA/EMA: If the current price is above the SMA or EMA, it may indicate an upward trend.
    data_copy['buy_signal'] = (
        (data_copy['future_return'] > price_threshold) &
        #(data_copy['RSI'] < 30) &   # buy signal for RSI
        (data_copy['RSI'] < 40) &    # buy signal for RSI
        ((data_copy['MACD'] > data_copy['Signal_Line']) & (data_copy['MACD'].shift(1) <= data_copy['Signal_Line'].shift(1))) & # buy signal for MACD
        ((data_copy['close'] > data_copy['SMA']) & (data_copy['close'].shift(1) <= data_copy['SMA'].shift(1))) # buy signal for SMA
    ).astype(int)

    data_copy['sell_signal'] = (
        (data_copy['future_return'] < - price_threshold) &
        #(data_copy['RSI'] < 30) &    # sell signal for RSI 
        (data_copy['RSI'] > 60) &     # sell signal for RSI 
        (data_copy['MACD'] < data_copy['Signal_Line']) & (data_copy['MACD'].shift(1) >= data_copy['Signal_Line'].shift(1)) & # sell signal for MACD 
        (data_copy['close'] < data_copy['SMA']) & (data_copy['close'].shift(1) >= data_copy['SMA'].shift(1)) # sell signal for SMA 
    ).astype(int)

    # Define the target as 1 (buy) if all conditions are met, and 0 (sell) if they are not. 
    # Convert `buy_signal` to an integer for binary classification.
    #data_copy['target'] = data_copy['buy_signal'].astype(int)

    # Initialize the 'target' column with default value
    data_copy['target'] = 0 # 'do nothing' signal
    
    # Fill the 'target' with the 'buy_signal' and 'sell_signal' conditions 
    data_copy.loc[(data_copy['buy_signal'] == 1) & (data_copy['sell_signal'] == 0), 'target'] = 1 # 'buy' signal
    data_copy.loc[(data_copy['sell_signal'] == 1) & (data_copy['buy_signal'] == 0), 'target'] = -1 # 'sell' signal

    # The last row in your dataset will have a NaN value for 'future_return' due to the shift operation. Drop this row to clean up the dataset.
    # data_copy = data_copy.dropna()

    # Check the balance of 1s and 0s in our target variable to understand how many “buy” and “sell” signals we have.
    print(data_copy['target'].value_counts())
    print(data_copy['buy_signal'].value_counts())
    print(data_copy['sell_signal'].value_counts())

    return data_copy


# Define the threshold for the price change to classify as 'buy' or 'sell'. For instance, if we want a 1% increase to be a 'buy' signal,
# the threshold will be 0.01.
threshold = 0.01

# Compute the 'target' variable
btc_price_data_1_year = create_target_variable_with_techn_indicators(btc_price_data_1_year, threshold)


btc_price_data_1_year.target.value_counts()





btc_price_data_1_year.describe().T


btc_price_data_1_year.dtypes


btc_price_data_1_year.shape


btc_price_data_1_year.isnull().sum()





btc_price_data_1_year = btc_price_data_1_year.dropna()


btc_price_data_1_year.shape


btc_price_data_1_year.isnull().sum()


btc_price_data_1_year.columns














attributes = btc_price_data_1_year.drop(columns=['target'])  # Features
target = btc_price_data_1_year['target']  # Target variable
attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False 
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape




















model = RandomForestClassifier(n_estimators=10, random_state=42)


model.fit(attributes_train, target_train)


model.estimator_


model.estimators_[0]


model.estimators_[0].tree_.max_depth


for estimator in model.estimators_:
    print(estimator.tree_.max_depth)





model.feature_importances_


model.feature_importances_.sum()


list(zip(btc_price_data_1_year.columns, np.round(model.feature_importances_, 3)))





#permutation_importance(model, attributes_train, target_train)


#permutation_importance(forest, attributes_digits, targets_digits)["importances_mean"]


result = list(zip(btc_price_data_1_year.columns, np.round(permutation_importance(model, attributes_train, target_train)["importances_mean"], 8)))


result


pd.DataFrame(result).sort_values(1, ascending = False)


#pd.DataFrame(result).sort_values(1, ascending = False)





model = RandomForestClassifier(n_estimators=10, random_state=42)


model_pipeline = Pipeline(
    [
        ("forestClass", RandomForestClassifier(n_estimators=10, random_state=42)),
        # ("scaler", MaxAbsScaler()),
        # ("svd", TruncatedSVD(n_components = 20)),
        # ("nb", SGDClassifier(loss = "hinge"))
    ]
)


# model_pipeline = Pipeline(
#     [
#         ("tfidf", TfidfVectorizer()),
#         ("scaler", MaxAbsScaler()),
#         ("svd", TruncatedSVD(n_components = 20)),
#         ("nb", SGDClassifier(loss = "hinge"))
#     ]
# )


# tuner = RandomizedSearchCV(model_pipeline, param_distributions = {
#     "tfidf__ngram_range" : [(1, 1), (1, 2)],
#     "svd__n_components" : [3, 10, 100, 150],
# }, n_iter = 5, verbose = 3)


# Use '.fit' only with the train data !!!
#tuner.fit(emails_train.text, labels_train)


# Check with train data
#print(classification_report(labels_train, tuner.predict(emails_train.text)))


# Check with test data
#print(classification_report(labels_test, tuner.predict(emails_test.text)))





experiment_name = "BTC + RandomForestClassifier"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None: 
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)


with mlflow.start_run(run_name = "Training dry run"):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))
    # mlflow.log_param("Stratification", stratify is not None)

    # model_pipeline.fit(emails_train.text, labels_train)
    model_pipeline.fit(attributes_train, target_train)
    
    # mlflow.log_param("vocab size", len(model_pipeline["tfidf"].vocabulary_))
    # mlflow.log_param("SVD n_components", model_pipeline["svd"].n_components)
    
    mlflow.log_metric("train_accuracy", model_pipeline.score(attributes_train, target_train))
    mlflow.log_metric("test_accuracy", model_pipeline.score(attributes_test, target_test))

    with open("train_report.txt",  "w") as f:
        # f.write(classification_report(labels_train, model_pipeline.predict(emails_train.text)))
        f.write(classification_report(target_train, model_pipeline.predict(attributes_train)))
    with open("test_report.txt",  "w") as f:
        # f.write(classification_report(labels_test, model_pipeline.predict(emails_test.text)))
        f.write(classification_report(target_test, model_pipeline.predict(attributes_test)))
        
    pickle.dump(model_pipeline, open("model.pkl", "wb"))

    mlflow.log_artifact("model.pkl")
    mlflow.log_artifact("train_report.txt")
    mlflow.log_artifact("test_report.txt")











predictions = model.predict(attributes_test)
print(predictions)


print("Unique classes:", np.unique(predictions))





# Check with train data
print(classification_report(target_train, model.predict(attributes_train)))


print(classification_report(target_test, model.predict(attributes_test)))


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
   
accuracy = accuracy_score(target_test, predictions)
precision = precision_score(target_test, predictions, average='macro')
recall = recall_score(target_test, predictions, average='macro')
f1 = f1_score(target_test, predictions, average='macro')
   
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')









