import numpy as np
import pandas as pd
# install pandas_ta - https://github.com/twopirllc/pandas-ta
import pandas_ta as ta

import matplotlib.pyplot as plt
import seaborn as sns

import pickle
import mlflow

from statsmodels.tsa.seasonal import STL, seasonal_decompose
from statsmodels.tsa.stattools import pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

from scipy.signal import periodogram

from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance

# Scalers
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Pipelines
from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer

# Metrics
from sklearn.metrics import classification_report, roc_auc_score

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier





























btc_price_data_1_year = pd.read_csv("data/bitcoin_historical_data_1_year.csv")
btc_price_data_1_year


btc_price_data_1_year.describe().T


btc_price_data_1_year.dtypes


btc_price_data_1_year.isnull().sum()











btc_price_data_1_year.timestamp = pd.to_datetime(btc_price_data_1_year.timestamp)





btc_price_data_1_year = btc_price_data_1_year.drop(columns = ["date", "time"])


btc_price_data_1_year.dtypes





btc_price_data_1_year.set_index('timestamp', inplace=True, drop = True)


# Ensure the data is sorted by time
btc_price_data_1_year = btc_price_data_1_year.sort_index()














#btc_price_data_1_year['year'] = btc_price_data_1_year.index.year
btc_price_data_1_year['month'] = btc_price_data_1_year.index.month
btc_price_data_1_year['day'] = btc_price_data_1_year.index.day
btc_price_data_1_year['weekday'] = btc_price_data_1_year.index.weekday  # Monday=0, Sunday=6
btc_price_data_1_year['hour'] = btc_price_data_1_year.index.hour





# Extract hour and weekday
btc_price_data_1_year['hour_sin'] = np.sin(2 * np.pi * btc_price_data_1_year.index.hour / 24)  # Hour sin encoding
btc_price_data_1_year['hour_cos'] = np.cos(2 * np.pi * btc_price_data_1_year.index.hour / 24)  # Hour cos encoding
btc_price_data_1_year['weekday_sin'] = np.sin(2 * np.pi * btc_price_data_1_year.index.weekday / 7)  # Weekday sin encoding
btc_price_data_1_year['weekday_cos'] = np.cos(2 * np.pi * btc_price_data_1_year.index.weekday / 7)  # Weekday cos encoding


btc_price_data_1_year.columns


btc_price_data_1_year.dtypes








# Apply STL decomposition
stl = STL(btc_price_data_1_year['close'], robust=True, period = 24)  # period=24 for daily seasonality in hourly data
result = stl.fit()


# Extract components
btc_price_data_1_year['trend'] = result.trend
btc_price_data_1_year['seasonal'] = result.seasonal
btc_price_data_1_year['residual'] = result.resid

# Plot the decomposition
result.plot()
plt.show()


# 1.Lagged Features - Lagged values of the trend, seasonal, and residual
for lag in range(1, 4):  # Use 1, 2, 3 hours as lags
    btc_price_data_1_year[f'trend_lag_{lag}'] = btc_price_data_1_year['trend'].shift(lag)
    btc_price_data_1_year[f'seasonal_lag_{lag}'] = btc_price_data_1_year['seasonal'].shift(lag)
    btc_price_data_1_year[f'residual_lag_{lag}'] = btc_price_data_1_year['residual'].shift(lag)

# 2.Rolling Statistics - Rolling mean and standard deviation of each component.
btc_price_data_1_year['trend_rolling_mean'] = btc_price_data_1_year['trend'].rolling(window=24).mean()
btc_price_data_1_year['residual_rolling_std'] = btc_price_data_1_year['residual'].rolling(window=24).std()

# 3.Differences - Calculate the difference between close price and its trend or seasonality
btc_price_data_1_year['close_trend_diff'] = btc_price_data_1_year['close'] - btc_price_data_1_year['trend']
btc_price_data_1_year['close_seasonal_diff'] = btc_price_data_1_year['close'] - btc_price_data_1_year['seasonal']


btc_price_data_1_year.shape


btc_price_data_1_year.columns











# We use the last 3 days (3 * 24 hours) for the autocorrelation analysis
lags = 72





plot_acf(btc_price_data_1_year.close, lags = lags)
plt.ylim(-1.2, 1.2)

plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('ACF for \'close\' price')
plt.show()


autocorr = [btc_price_data_1_year['close'].autocorr(lag) for lag in range(1, lags+1)]
sns.heatmap(pd.DataFrame(autocorr, columns=["Autocorrelation"]).T, annot=False, cmap="coolwarm")
plt.xlabel('Lag')
plt.title('Heatmap of Autocorrelation for \'close\' price')
plt.show()








# Calculate PACF
pacf_values = pacf(btc_price_data_1_year['close'], nlags=lags)

# Calculate the confidence interval
conf_int = 1.96 / np.sqrt(len(btc_price_data_1_year['close']))  # The 95% confidence interval

# Get significant lags (values outside the confidence interval)
significant_lags = np.where(np.abs(pacf_values) > conf_int)[0]

# Plot PACF with significant lags marked
plt.stem(pacf_values)

plt.axhline(y=conf_int, color='r', linestyle='--')
plt.axhline(y=-conf_int, color='r', linestyle='--')
plt.ylim(-0.010, 0.010)

plt.grid()

plt.xlabel('Lag')
plt.ylabel('Partial autocorrelation coefficient(PACF)')
plt.title("PACF with Significant Lags for \'close\' Price'")
plt.show()

# Print the significant lags
print("Significant Lags:", significant_lags)





# Remove the autocorrelation with itself (the '0'-th lag)
significant_lags = significant_lags[1:] if significant_lags[0] == 0 else significant_lags
print("Significant Lags:", significant_lags)


# Create lag features (past values as features)
def create_lag_features(df, lags = []):
    """
    Create lag features (past values as features) for supervised learning
    """
    # Create a copy of the DataFrame
    df_copy = df.copy(deep=True)
    
    for lag in lags:
        df_copy[f'lag_{lag}'] = df_copy['close'].shift(lag)
        
    #df.dropna(inplace=True)
    return df_copy


btc_price_data_1_year = create_lag_features(btc_price_data_1_year, significant_lags)


btc_price_data_1_year.shape


btc_price_data_1_year.columns














def calculate_rsi(data, window=14):
    """
    RSI is a momentum oscillator that measures the speed and change of price movements, typically over a 14-period interval.
    Assuming 'data' is a DataFrame with a 'close' price column

    Parameters
    ----------
    data:   a DataFrame with the time series data. A column with the name 'close' must be present in the DataFrame! 
            This column is used to calculate the 'RSI' value.
    window: the time period that is taken into account when calculating the 'RSI'
    """
    delta = data['close'].diff(1)
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    
    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = loss.rolling(window=window, min_periods=1).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    data['RSI'] = rsi
    
    return data


# Calculate RSI
btc_price_data_1_year = calculate_rsi(btc_price_data_1_year)








def calculate_macd(data, short_window=12, long_window=26, signal_window=9):
    """
    Moving Average Convergence Divergence (MACD) is calculated using two exponential moving averages (EMA):
    the 12-day EMA and the 26-day EMA, with a 9-day EMA as the signal line.
    """
    data['EMA12'] = data['close'].ewm(span=short_window, adjust=False).mean()
    data['EMA26'] = data['close'].ewm(span=long_window, adjust=False).mean()
    
    # MACD Line
    data['MACD'] = data['EMA12'] - data['EMA26']
    
    # Signal Line
    data['Signal_Line'] = data['MACD'].ewm(span=signal_window, adjust=False).mean()
    
    return data


# Calculate MACD (Moving Average Convergence Divergence)
btc_price_data_1_year = calculate_macd(btc_price_data_1_year)








def calculate_moving_averages(data, sma_window=20, ema_window=20):
    """
    Simple Moving Average (SMA) is the average price over a specified number of periods, 
    while Exponential Moving Average (EMA) gives more weight to recent prices.
    """
    # Simple Moving Average
    data['SMA'] = data['close'].rolling(window=sma_window).mean()
    
    # Exponential Moving Average
    data['EMA'] = data['close'].ewm(span=ema_window, adjust=False).mean()
    
    return data


# Calculate moving averages
btc_price_data_1_year = calculate_moving_averages(btc_price_data_1_year)





def create_target_variable(data, threshold = 0.01):
    """
    Computes and sets the 'target' variable from the input 'data' and 'threshold'.
    Creates a 'target' column with the computed values in the 'data' DataFrame.

    Parameters
    ----------
    data: a DataFrame with the time series data. There must be a column named 'close'! 
          This column will be used by the user to calculate the 'target' variable.
    
    threshold: threshold for the price change to classify as 'buy' or 'sell'. For instance, if you want a 1% increase to be a 'buy' signal,
               the threshold will be 0.01. Adjust this threshold as per your strategy.
    """
    # Create a copy of the DataFrame
    data_copy = data.copy(deep=True)
    
    # Compute the percentage change between the current close price and the close price in the next period.
    # This will help define whether there’s a significant increase or decrease.
    data_copy['future_return'] = ((data_copy['close'].shift(-1) - data_copy['close']) / data_copy['close']) * 100

    # Define the target as 1 (buy) if the future return is above the threshold, and 0 (sell) if it is below or equal to the threshold.
    data_copy['target'] = (data_copy['future_return'] > threshold).astype(float)

    # The last row in your dataset will have a NaN value for 'future_return' due to the shift operation. Drop this row to clean up the dataset.
    #data_copy = data_copy.dropna()

    # Check the balance of 1s and 0s in our target variable to understand how many “buy” and “sell” signals we have.
    print(data_copy['target'].value_counts())

    return data_copy


def create_target_variable_with_techn_indicators(data, price_threshold = 0.01):
    """
    Computes and sets the 'target' variable from the input 'data' and 'threshold'.
    Creates a 'target' column with the computed values in the 'data' DataFrame.

    Parameters
    ----------
    data: a DataFrame with the time series data. There must be a column named 'close'! 
          This column will be used by the user to calculate the 'target' variable.
    
    threshold: threshold for the price change to classify as 'buy' or 'sell'. For instance, if you want a 1% increase to be a 'buy' signal,
               the threshold will be 0.01. Adjust this threshold as per your strategy.
    """
    # Create a copy of the DataFrame
    data_copy = data.copy(deep=True)
    
    # Compute the percentage change between the current close price and the close price in the next period.
    # This will help define whether there’s a significant increase or decrease.
    data_copy['future_return'] = ((data_copy['close'].shift(-1) - data_copy['close']) / data_copy['close']) * 100
    
    # Use the technical indicators to add more conditions to the target:
    # - RSI: A Relative Strength Index (RSI) value below 30 often indicates an oversold condition, which might suggest a buying opportunity.
    # - MACD: A positive MACD value (i.e., MACD > Signal Line) can suggest an uptrend.
    # - SMA/EMA: If the current price is above the SMA or EMA, it may indicate an upward trend.
    #data_copy['buy_signal'] = (
    conditions_buy  = (
        (data_copy['future_return'] > price_threshold) &
        # (data_copy['RSI'] < 30) &   # buy signal for RSI
        (data_copy['RSI'] < 40) &    # buy signal for RSI
        ((data_copy['MACD'] > data_copy['Signal_Line']) & (data_copy['MACD'].shift(1) <= data_copy['Signal_Line'].shift(1))) & # buy signal for MACD
        ((data_copy['close'] > data_copy['SMA']) & (data_copy['close'].shift(1) <= data_copy['SMA'].shift(1))) # buy signal for SMA
    ).astype(int)

    conditions_sell  = (
        (data_copy['future_return'] < - price_threshold) &
        #(data_copy['RSI'] > 70) &    # sell signal for RSI 
        (data_copy['RSI'] > 60) &     # sell signal for RSI 
        (data_copy['MACD'] < data_copy['Signal_Line']) & (data_copy['MACD'].shift(1) >= data_copy['Signal_Line'].shift(1)) & # sell signal for MACD 
        (data_copy['close'] < data_copy['SMA']) & (data_copy['close'].shift(1) >= data_copy['SMA'].shift(1)) # sell signal for SMA 
    ).astype(int)

    # Define the 'target' as 1 (buy), 0(do nothing) and -1(sell).
    # Initialize the 'target' column with default value
    data_copy['target'] = 0 # 'do nothing' signal
    
    # Fill the 'target' with the 'buy_signal' and 'sell_signal' conditions 
    data_copy.loc[(conditions_buy == 1) & (conditions_sell == 0), 'target'] = 1 # 'buy' signal
    data_copy.loc[(conditions_sell == 1) & (conditions_buy == 0), 'target'] = -1 # 'sell' signal

    # Generate signals
    # data['signal'] = np.where(data['SMA_short'] > data['SMA_long'], 1, 0)
    # data['signal'] = data['signal'].diff()  # 1 = Buy, -1 = Sell

    # The last row in your dataset will have a NaN value for 'future_return' due to the shift operation. Drop this row to clean up the dataset.
    # data_copy = data_copy.dropna()

    # Print the balance of 1s, 0s and -1s in our 'target' variable to understand how many 'buy' and 'sell' signals we have.
    print(data_copy['target'].value_counts())

    return data_copy


def create_target_variable_with_techn_indicators_hourly(data, price_threshold = 0.01):
    """
    Computes and sets the 'target' variable from the input 'data' and 'threshold'.
    Creates a 'target' column with the computed values in the 'data' DataFrame.

    Parameters
    ----------
    data: a DataFrame with the time series data. There must be a column named 'close'! 
          This column will be used by the user to calculate the 'target' variable.
    
    threshold: threshold for the price change to classify as 'buy' or 'sell'. For instance, if you want a 1% increase to be a 'buy' signal,
               the threshold will be 0.01. Adjust this threshold as per your strategy.
    """
    # Create a copy of the DataFrame
    data_copy = data.copy(deep=True)
    
    # Compute the percentage change between the current close price and the close price in the next period.
    # This will help define whether there’s a significant increase or decrease.
    data_copy['future_return'] = ((data_copy['close'].shift(-1) - data_copy['close']) / data_copy['close']) * 100
    
    # Use the technical indicators to add more conditions to the target:
    # - RSI: A Relative Strength Index (RSI) value below 30 often indicates an oversold condition, which might suggest a buying opportunity.
    # - MACD: A positive MACD value (i.e., MACD > Signal Line) can suggest an uptrend.
    # - SMA/EMA: If the current price is above the SMA or EMA, it may indicate an upward trend.
    #data_copy['buy_signal'] = (
    conditions_buy  = (
        (data_copy['future_return'] > price_threshold) &
        #(data_copy['RSI'] < 30) &   # buy signal for RSI
        (data_copy['RSI'] < 40) &    # buy signal for RSI
        ((data_copy['MACD'] > data_copy['Signal_Line']) & (data_copy['MACD'].shift(1) <= data_copy['Signal_Line'].shift(1))) #& # buy signal for MACD
        # ((data_copy['close'] > data_copy['SMA']) & (data_copy['close'].shift(1) <= data_copy['SMA'].shift(1))) # buy signal for SMA
    ).astype(int)

    conditions_sell  = (
        (data_copy['future_return'] < - price_threshold) &
        #(data_copy['RSI'] > 70) &    # sell signal for RSI 
        (data_copy['RSI'] > 60) &     # sell signal for RSI 
        (data_copy['MACD'] < data_copy['Signal_Line']) & (data_copy['MACD'].shift(1) >= data_copy['Signal_Line'].shift(1)) #& # sell signal for MACD 
        # (data_copy['close'] < data_copy['SMA']) & (data_copy['close'].shift(1) >= data_copy['SMA'].shift(1)) # sell signal for SMA 
    ).astype(int)

    # Define the 'target' as 1 (buy), 0(do nothing) and -1(sell).
    # Initialize the 'target' column with default value
    data_copy['target'] = 0 # 'do nothing' signal
    
    # Fill the 'target' with the 'buy_signal' and 'sell_signal' conditions 
    data_copy.loc[(conditions_buy == 1) & (conditions_sell == 0), 'target'] = 1 # 'buy' signal
    data_copy.loc[(conditions_sell == 1) & (conditions_buy == 0), 'target'] = -1 # 'sell' signal

    # Generate signals
    # data['signal'] = np.where(data['SMA_short'] > data['SMA_long'], 1, 0)
    # data['signal'] = data['signal'].diff()  # 1 = Buy, -1 = Sell

    # The last row in your dataset will have a NaN value for 'future_return' due to the shift operation. Drop this row to clean up the dataset.
    # data_copy = data_copy.dropna()

    # Print the balance of 1s, 0s and -1s in our 'target' variable to understand how many 'buy' and 'sell' signals we have.
    print(data_copy['target'].value_counts())

    return data_copy


# Define the threshold for the price change to classify as 'buy' or 'sell'. For instance, if we want a 1% increase to be a 'buy' signal,
# the threshold will be 0.01.
threshold = 0.01

# Compute the 'target' variable
btc_price_data_1_year = create_target_variable_with_techn_indicators_hourly(btc_price_data_1_year, threshold)





btc_price_data_1_year.describe().T


btc_price_data_1_year.dtypes


btc_price_data_1_year.shape


btc_price_data_1_year.isnull().sum()





btc_price_data_1_year = btc_price_data_1_year.dropna()


btc_price_data_1_year.shape


btc_price_data_1_year.isnull().sum()


btc_price_data_1_year.columns





btc_price_data_1_year.shape


btc_price_data_1_year.target.value_counts()


# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt

# Load your Bitcoin dataset (assume columns: 'timestamp', 'close', 'high', 'low', 'volume')
# df = pd.read_csv('bitcoin_price.csv')
# df['timestamp'] = pd.to_datetime(df['timestamp'])
# df.set_index('timestamp', inplace=True)

df = btc_price_data_1_year

# # Resample the data to different timeframes
# def resample_data(data, timeframe='15T'):
#     return data.resample(timeframe).mean()

    # return data.resample(timeframe).agg({
    #     'close': 'last', 
    #     'high': 'max', 
    #     'low': 'min', 
    #     'volume': 'sum'
    # }).dropna()

# # Apply a simple moving average crossover strategy
# def sma_crossover_strategy(data, short_window=10, long_window=50):
#     data['SMA_short'] = data['close'].rolling(window=short_window).mean()
#     data['SMA_long'] = data['close'].rolling(window=long_window).mean()
    
#     # Generate signals
#     data['signal'] = np.where(data['SMA_short'] > data['SMA_long'], 1, 0)
#     data['signal'] = data['signal'].diff()  # 1 = Buy, -1 = Sell
#     return data

# Backtest the strategy
def backtest(data, initial_balance=1000):
    balance = initial_balance
    position = 0  # Number of Bitcoin held
    count = 0
    for i in range(1, len(data)):
        # print("target: ", data.target)
        if data['target'].iloc[i] == 1 and balance > 0:  # Buy signal
            count = count + 1
            #print("count buy:", count)
            
           # print("Buy signal balance:", balance)
            position = balance / data['close'].iloc[i]
           # print("Buy signal position:", position)
           # print("--------------------------------")
            balance = 0
        elif data['target'].iloc[i] == -1 and position > 0:  # Sell signal
            count = count + 1
           # print("count sell:", count)
            
           # print("Sell signal position:", position)
            balance = position * data['close'].iloc[i]
          #  print("Sell signal balance:", balance)
          #  print("--------------------------------")
            position = 0
    # Final portfolio value
    final_balance = balance + (position * data['close'].iloc[-1])
    print("final_balance after:", final_balance)
    return final_balance

# Test on multiple timeframes
timeframes = [ '1h']
# timeframes = ['1min', '5min', '15min', '1h', '4h', '1D']
results = {}

for timeframe in timeframes:
    # df_resampled = resample_data(df, timeframe)
    # df_strategy = sma_crossover_strategy(df_resampled)
    # final_balance = backtest(df_strategy)
    # print(df_resampled.shape)
    # Compute the 'target' variable
    # df_strategy = create_target_variable_with_techn_indicators(df_resampled, threshold)
    final_balance = backtest(df)
    results[timeframe] = final_balance

# Print results
for timeframe, balance in results.items():
    print(f"Final Balance with {timeframe} data: ${balance:.2f}")
    final_return = ((balance - 1000) / 1000) * 100
    print(f"Final Return with {timeframe} data: ${final_return:.2f}[%]")

# Visualize a selected timeframe (e.g., 15-minute data)
# df_strategy = sma_crossover_strategy(resample_data(df, '15T'))
# df_strategy = resample_data(df, '1h')
plt.figure(figsize=(12, 6))
# plt.plot(df_strategy['close'], label='Close Price')
plt.plot(df['MACD'][:500], label='MACD', linestyle='-')
plt.plot(df['Signal_Line'][:500], label='Signal_Line', linestyle='--')
plt.legend()
plt.title('SMA Crossover Strategy on 1-Minute Data')
plt.show()








btc_price_data_1_year


btc_price_data_1_year.columns








from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

# Assuming your data is in `df`
df = btc_price_data_1_year.copy(deep = True)

X = df.drop(columns=['target'])  # Replace 'target' with your target column name
y = df['target']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = False)

# Define pipeline with placeholder for scaler and model
pipeline = Pipeline([
    ('scaler', None),  # Placeholder for scaler
    ('rf', RandomForestClassifier(random_state=42))
])

# Define parameter grid for grid search
# param_grid = {
#     'scaler': [None, MinMaxScaler(), StandardScaler()],  # Test with and without scaling
#     'rf__n_estimators': [100, 300],                # Number of trees
#     # 'rf__n_estimators': [100, 300, 500],                # Number of trees
#     'rf__max_depth': [None, 10, 20, 30],                # Tree depth
#     'rf__min_samples_split': [2, 5, 10],                # Min samples required to split
#     'rf__min_samples_leaf': [1, 2, 4],                  # Min samples required at leaf node
#     'rf__max_features': ['sqrt', 'log2', None],         # Max features considered for split
#     'rf__bootstrap': [True, False]                      # Use bootstrap sampling or not
# }

param_grid = {
    'scaler': [MinMaxScaler(), StandardScaler(), None],  # Test multiple scalers
    'rf__n_estimators': [100, 300],
    'rf__max_depth': [10, 20, None],
    'rf__min_samples_split': [2, 5, 10],
}

# param_grid = {
#     'scaler': [None, MinMaxScaler(), StandardScaler()],  # Test with and without scaling
#     'rf__n_estimators': [100, 300],                # Number of trees
#     # 'rf__n_estimators': [100, 300, 500],                # Number of trees
#     'rf__max_depth': [None, 10, 20, 30],                # Tree depth
#     'rf__min_samples_split': [2, 5, 10],                # Min samples required to split
#     'rf__min_samples_leaf': [1, 2, 4],                  # Min samples required at leaf node
#     'rf__max_features': ['sqrt', 'log2', None],         # Max features considered for split
#     'rf__bootstrap': [True, False]                      # Use bootstrap sampling or not
# }

# Perform grid search
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=2,                # 3-fold cross-validation
    # cv=3,                # 3-fold cross-validation
    scoring='accuracy',  # Optimize for accuracy
    verbose = 5,
    n_jobs=9          
    # n_jobs=-1            # Use all available CPU cores
)

# Fit grid search
# grid_search.fit(X_train, y_train)


# # Output best parameters and score
# print("Best parameters:", grid_search.best_params_)
# print("Best accuracy:", grid_search.best_score_)

# # Evaluate on test set
# best_model = grid_search.best_estimator_
# test_score = best_model.score(X_test, y_test)
# print("Test set accuracy:", test_score)









attributes = btc_price_data_1_year.drop(columns=['target'])  # Features
target = btc_price_data_1_year['target']  # Target variable


attributes.columns





#btc_price_data_1_year








attributes = btc_price_data_1_year.drop(columns=['target'])  # Features
target = btc_price_data_1_year['target']  # Target variable
attributes_train, attributes_test, target_train, target_test = train_test_split(
    attributes, 
    target, 
    test_size=0.2, 
    shuffle = False,
    random_state = 42
)


attributes_train.shape, attributes_test.shape


target_train.shape, target_test.shape











#btc_price_data_1_year[attributes.columns] = scaler.fit_transform(btc_price_data_1_year[attributes.columns])


# from sklearn.preprocessing import MinMaxScaler

# scaler = MinMaxScaler()
# # features = ['trend', 'seasonal', 'residual', 'trend_rolling_mean', 'residual_rolling_std']
# btc_price_data_1_year[features.columns] = scaler.fit_transform(btc_price_data_1_year[features.columns])


# from sklearn.preprocessing import StandardScaler

# scaler = StandardScaler()
# # features = ['trend', 'seasonal', 'residual', 'trend_rolling_mean', 'residual_rolling_std']
# btc_price_data_1_year[attributes.columns] = scaler.fit_transform(btc_price_data_1_year[attributes.columns])











model = RandomForestClassifier(random_state=42, class_weight='balanced')
# model = RandomForestClassifier(n_estimators = 300, max_depth = 10, min_samples_split = 2, random_state=42)
# model = RandomForestClassifier(n_estimators=10, random_state=42)


model.fit(attributes_train, target_train)


model.estimator_


model.estimators_[0]


model.estimators_[0].tree_.max_depth


for estimator in model.estimators_:
    print(estimator.tree_.max_depth)





model.feature_importances_


# Visualize feature importances
def plot_feature_importances(model, scaler_name = 'StandardScaler'):
    
    # print(f"Feature Importances for {scaler_name}:")
    importances = model.feature_importances_
    sorted_indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.bar(range(attributes_train.shape[1]), importances[sorted_indices], align='center')
    plt.xticks(range(attributes_train.shape[1]), attributes.columns[sorted_indices], rotation=90)
    plt.title(f"Feature Importances ({scaler_name})")
    plt.tight_layout()
    plt.show()



plot_feature_importances(model)



model.feature_importances_.sum()


result_feat_imp = list(zip(btc_price_data_1_year.columns, np.round(model.feature_importances_, 3)))


pd.DataFrame(result_feat_imp).sort_values(1, ascending = False)





#permutation_importance = permutation_importance(model, attributes_train, target_train)


#permutation_importance(forest, attributes_digits, targets_digits)["importances_mean"]


# result_permut_imp = list(zip(btc_price_data_1_year.columns, np.round(permutation_importance(model, attributes_train, target_train)["importances_mean"], 8)))
#result_permut_imp = list(zip(btc_price_data_1_year.columns, np.round(permutation_importance["importances_mean"], 8)))


#result_permut_imp


# Visualize feature importances
def plot_permutation_importances(importances , scaler_name = 'StandardScaler'):
    
    # print(f"Feature Importances for {scaler_name}:")
    # importances = model.feature_importances_
    sorted_indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.bar(range(attributes_train.shape[1]), importances[sorted_indices], align='center')
    plt.xticks(range(attributes_train.shape[1]), attributes.columns[sorted_indices], rotation=90)
    plt.title(f"Feature Importances ({scaler_name})")
    plt.tight_layout()
    plt.show()


#plot_permutation_importances(result_permut_imp)


#pd.DataFrame(result_permut_imp).sort_values(1, ascending = False)


#pd.DataFrame(result_feat_imp).sort_values(1, ascending = False)


#pd.DataFrame(result).sort_values(1, ascending = False)











# from sklearn.ensemble import RandomForestClassifier
# from sklearn.preprocessing import MinMaxScaler, StandardScaler
# from sklearn.pipeline import Pipeline
# from sklearn.model_selection import GridSearchCV
# from sklearn.model_selection import train_test_split

# # Assuming your data is in `df`
# df = btc_price_data_1_year.copy(deep = True)

# X = df.drop(columns=['target'])  # Replace 'target' with your target column name
# y = df['target']

# # Train-test split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = false)

# # Define pipeline with placeholder for scaler and model
# pipeline = Pipeline([
#     ('scaler', None),  # Placeholder for scaler
#     ('rf', RandomForestClassifier(random_state=42))
# ])

# # Define parameter grid for grid search
# param_grid = {
#     'scaler': [None, MinMaxScaler(), StandardScaler()],  # Test with and without scaling
#     'rf__n_estimators': [100, 300],                # Number of trees
#     # 'rf__n_estimators': [100, 300, 500],                # Number of trees
#     'rf__max_depth': [None, 10, 20, 30],                # Tree depth
#     'rf__min_samples_split': [2, 5, 10],                # Min samples required to split
#     'rf__min_samples_leaf': [1, 2, 4],                  # Min samples required at leaf node
#     'rf__max_features': ['sqrt', 'log2', None],         # Max features considered for split
#     'rf__bootstrap': [True, False]                      # Use bootstrap sampling or not
# }

# # Perform grid search
# grid_search = GridSearchCV(
#     pipeline,
#     param_grid,
#     cv=2,                # 3-fold cross-validation
#     # cv=3,                # 3-fold cross-validation
#     scoring='accuracy',  # Optimize for accuracy
#     verbose=2,
#     n_jobs=-9          
#     # n_jobs=-1            # Use all available CPU cores
# )

# # Fit grid search
# grid_search.fit(X_train, y_train)

# # Output best parameters and score
# print("Best parameters:", grid_search.best_params_)
# print("Best accuracy:", grid_search.best_score_)

# # Evaluate on test set
# best_model = grid_search.best_estimator_
# test_score = best_model.score(X_test, y_test)
# print("Test set accuracy:", test_score)












#model = RandomForestClassifier(n_estimators=10, random_state=42)


model_pipeline = Pipeline(
    [
        ("scaler", MinMaxScaler()),
        ("forest", RandomForestClassifier(class_weight='balanced', n_estimators = 300, max_depth = 10, min_samples_split = 2, random_state=42)),
        # ("forestClass", RandomForestClassifier(n_estimators=10, min_samples_split= 50, random_state=42)),
        # ("scaler", StandardScaler()),
        # ("svd", TruncatedSVD(n_components = 20)),
        # ("nb", SGDClassifier(loss = "hinge"))
    ]
)


# model_pipeline = Pipeline(
#     [
#         ("tfidf", TfidfVectorizer()),
#         ("scaler", MaxAbsScaler()),
#         ("svd", TruncatedSVD(n_components = 20)),
#         ("nb", SGDClassifier(loss = "hinge"))
#     ]
# )


# tuner = RandomizedSearchCV(model_pipeline, param_distributions = {
#     "tfidf__ngram_range" : [(1, 1), (1, 2)],
#     "svd__n_components" : [3, 10, 100, 150],
# }, n_iter = 5, verbose = 3)


# Use '.fit' only with the train data !!!
#tuner.fit(emails_train.text, labels_train)


# Check with train data
#print(classification_report(labels_train, tuner.predict(emails_train.text)))


# Check with test data
#print(classification_report(labels_test, tuner.predict(emails_test.text)))





experiment_name = "BTC + RandomForestClassifier + parameter fine tuning"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None: 
    mlflow.create_experiment(experiment_name)


mlflow.set_experiment(experiment_name)


with mlflow.start_run(run_name = "Training Random Forest - balanced "):
    mlflow.log_param("train size", len(attributes_train))
    mlflow.log_param("test size", len(attributes_test))
    # mlflow.log_param("Stratification", stratify is not None)

    # model_pipeline.fit(emails_train.text, labels_train)
    model_pipeline.fit(attributes_train, target_train)
    
    # mlflow.log_param("vocab size", len(model_pipeline["tfidf"].vocabulary_))
    # mlflow.log_param("SVD n_components", model_pipeline["svd"].n_components)
    
    mlflow.log_metric("train_accuracy", model_pipeline.score(attributes_train, target_train))
    mlflow.log_metric("test_accuracy", model_pipeline.score(attributes_test, target_test))

    with open("train_report.txt",  "w") as f:
        # f.write(classification_report(labels_train, model_pipeline.predict(emails_train.text)))
        f.write(classification_report(target_train, model_pipeline.predict(attributes_train)))
    with open("test_report.txt",  "w") as f:
        # f.write(classification_report(labels_test, model_pipeline.predict(emails_test.text)))
        f.write(classification_report(target_test, model_pipeline.predict(attributes_test)))
        
    pickle.dump(model_pipeline, open("model.pkl", "wb"))

    mlflow.log_artifact("model.pkl")
    mlflow.log_artifact("train_report.txt")
    mlflow.log_artifact("test_report.txt")


from sklearn.metrics import precision_score

preds = model_pipeline.predict(attributes_test)


np.unique(preds)


preds = pd.Series(preds, index = attributes_test.index)


precision_score(target_test, preds, average='micro') 


combined = pd.concat([target_test, preds], axis = 1)


combined


combined.plot()








predictions = model.predict(attributes_test)
print(predictions)


print("Unique classes:", np.unique(predictions))





# Check with train data
print(classification_report(target_train, model.predict(attributes_train)))


print(classification_report(target_test, model.predict(attributes_test)))


target_test.value_counts()


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
   
accuracy = accuracy_score(target_test, predictions)
precision = precision_score(target_test, predictions, average='macro')
recall = recall_score(target_test, predictions, average='macro')
f1 = f1_score(target_test, predictions, average='macro')
   
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')









